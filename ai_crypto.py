# -*- coding: utf-8 -*-
"""AI_Crypto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LRT0TO2Wr-ghgULcVBL09QL4tU2IXbq0

# Data Preprocessing

## Importing the libraries
"""

import json
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import random

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from keras import regularizers

import warnings
warnings.filterwarnings('ignore')

"""## Importing the dataset


"""

df = pd.read_csv("new_dataset_15k.csv", sep=';', decimal=',', encoding='cp1251')
df.drop(columns=[col for col in df.columns if 'Unnamed:' in col], inplace=True, errors='ignore')

price_cols = ['outcome_price', 'initial_entry_price']
all_potential_feature_cols = [
    col for col in df.columns if
    col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value'
]

if 'feat_market_state' in df.columns and df['feat_market_state'].dtype == 'object':
    df['feat_market_state'] = df['feat_market_state'].map(
        {'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}
    ).fillna(0)

cols_to_convert_numeric = price_cols + all_potential_feature_cols
for col_name in list(set(cols_to_convert_numeric)):
    if col_name in df.columns:
        df[col_name] = pd.to_numeric(
            df[col_name].astype(str).str.replace(',', '.'), errors='coerce'
        )

df.dropna(subset=price_cols + ['outcome_status'], inplace=True)
if df.empty:
    raise ValueError("DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN –≤ —Ü–µ–Ω–∞—Ö/—Å—Ç–∞—Ç—É—Å–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")

"""##Feature Engineering"""

base_feature_columns = [
    col for col in all_potential_feature_cols
    if col in df.columns and pd.api.types.is_numeric_dtype(df[col])
]
if 'feat_market_state' in df.columns and \
   pd.api.types.is_numeric_dtype(df['feat_market_state']) and \
   'feat_market_state' not in base_feature_columns:
    base_feature_columns.append('feat_market_state')
base_feature_columns = list(set(base_feature_columns))

df[base_feature_columns] = df[base_feature_columns].fillna(0)

engineered_feature_columns = base_feature_columns.copy()
lags = [1, 2, 4, 8]
roll_windows = [4, 8]
new_cols_data = {}

for col in base_feature_columns:
    for lag in lags:
        new_cols_data[f'{col}_lag{lag}'] = df[col].shift(lag)
    for window in roll_windows:
        new_cols_data[f'{col}_roll_mean{window}'] = df[col].rolling(window=window, min_periods=1).mean()

df = pd.concat([df, pd.DataFrame(new_cols_data, index=df.index)], axis=1)
df.dropna(inplace=True)

if df.empty:
    raise ValueError("DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ Feature Engineering –∏ dropna. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∞–≥–∏/–æ–∫–Ω–∞.")

feature_columns = list(set(engineered_feature_columns + list(new_cols_data.keys())))

"""## Formation of a sample of "strong movements" and final X, y"""

df['price_change'] = (df['outcome_price'] - df['initial_entry_price']) / df['initial_entry_price'] * 100
strong_mask = (((df['outcome_status'] == 'LONG_MET') & (df['price_change'] > 0.5)) |
               ((df['outcome_status'] == 'SHORT_MET') & (df['price_change'] < -0.5)))
strong_moves = df.loc[strong_mask].copy()

if strong_moves.empty:
    raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ 'strong_moves'.")

actual_feature_cols_in_strong = [col for col in feature_columns if col in strong_moves.columns]
X_prepared = strong_moves[actual_feature_cols_in_strong].copy()
X_prepared.fillna(0, inplace=True)

valid_cols = [col for col in actual_feature_cols_in_strong if X_prepared[col].std() > 1e-6]
if not valid_cols:
    raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö (–Ω–µ–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö) –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")

X_before_selection = X_prepared[valid_cols].copy()
y_labels = (strong_moves['outcome_status'] == 'LONG_MET').astype(int)

print(f"–†–∞–∑–º–µ—Ä—ã –ø–µ—Ä–µ–¥ –æ—Ç–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: X={X_before_selection.shape}, y={y_labels.shape}")

"""## Splitting the dataset into the Training set and Test set"""

scaler = StandardScaler()
X_scaled_all_features = scaler.fit_transform(X_before_selection)
X_scaled_all_features = np.nan_to_num(X_scaled_all_features, nan=0.0, posinf=0.0, neginf=0.0)

NUM_FEATURES_TO_SELECT = 80
print(f"–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_scaled_all_features.shape[1]} –¥–æ {NUM_FEATURES_TO_SELECT}...")

selector = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    max_features=NUM_FEATURES_TO_SELECT,
    threshold=-np.inf
)

selector.fit(X_scaled_all_features, y_labels)
X_selected_features = selector.transform(X_scaled_all_features)

selected_feature_indices = selector.get_support(indices=True)
selected_feature_names = X_before_selection.columns[selected_feature_indices].tolist()

print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞: {X_selected_features.shape[1]}")
if 0 < X_selected_features.shape[1] <= 10: # –ü–æ–∫–∞–∑–∞—Ç—å –∏–º–µ–Ω–∞, –µ—Å–ª–∏ –∏—Ö –Ω–µ–º–Ω–æ–≥–æ
    print(f"–ü—Ä–∏–º–µ—Ä –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {selected_feature_names[:min(10, len(selected_feature_names))]}")
elif X_selected_features.shape[1] > 10:
     print(f"–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 5 –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {selected_feature_names[:5]}")


if X_selected_features.shape[1] == 0:
    raise ValueError("–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–≤–∏–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã SelectFromModel.")
if X_selected_features.shape[1] != NUM_FEATURES_TO_SELECT:
    print(f"–í–ù–ò–ú–ê–ù–ò–ï: –û—Ç–æ–±—Ä–∞–Ω–æ {X_selected_features.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ö–æ—Ç—è –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–æ—Å—å {NUM_FEATURES_TO_SELECT}. "
          "–≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –µ—Å–ª–∏ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–Ω—å—à–µ –∏–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é.")

X_temp, X_test, y_temp, y_test = train_test_split(
    X_selected_features, y_labels, test_size=0.3, random_state=42, stratify=y_labels
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp
)

print(f"–†–∞–∑–º–µ—Ä—ã –∏—Ç–æ–≥–æ–≤—ã—Ö –≤—ã–±–æ—Ä–æ–∫:")
print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"y_train: 0={np.sum(y_train==0)}, 1={np.sum(y_train==1)}")
print(f"y_val:   0={np.sum(y_val==0)}, 1={np.sum(y_val==1)}")
print(f"y_test:  0={np.sum(y_test==0)}, 1={np.sum(y_test==1)}")

"""# AI Create

## Training model
"""

MODEL_SEED = 10
np.random.seed(MODEL_SEED)
tf.random.set_seed(MODEL_SEED)
print(f"–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ê–ù–°–ê–ú–ë–õ–Ø –° MODEL_SEED = {MODEL_SEED}")
# -------------------------------------------------------

model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],),
                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.35),

    tf.keras.layers.Dense(128, activation='relu',
                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.35),

    tf.keras.layers.Dense(64, activation='relu',
                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Dense(32, activation='relu',
                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Dense(16, activation='relu',
                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Dense(1, activation='sigmoid')
])

class_weight = {0: 1.0, 1: 2.5}

LEARNING_RATE = 0.001

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_save_dir = "saved_ensemble_models"
if not os.path.exists(model_save_dir):
    os.makedirs(model_save_dir)

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=90,
        restore_best_weights=True,
        verbose=1,
        mode='max'
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=30,
        min_lr=0.00001,
        verbose=1,
        mode='min'
    )
]

print(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–Ω—Å–∞–º–±–ª—è (SEED={MODEL_SEED})...")

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    class_weight=class_weight,
    callbacks=callbacks,
    verbose=1
)

max_val_acc_this_run = 0
if hasattr(history, 'history') and history.history and 'val_accuracy' in history.history:
    max_val_acc_this_run = max(history.history['val_accuracy'])
print(f"Max val_accuracy –¥–ª—è SEED={MODEL_SEED}: {max_val_acc_this_run:.4f}")

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test accuracy –¥–ª—è SEED={MODEL_SEED}: {test_acc:.4f}")

test_pred_proba = model.predict(X_test, verbose=0).flatten()
confident_predictions = np.sum((test_pred_proba > 0.7) | (test_pred_proba < 0.3))
print(f"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ: {confident_predictions}/{len(test_pred_proba)} ({confident_predictions/len(test_pred_proba)*100:.1f}%)")

model_filename = f"ensemble_model_S{MODEL_SEED}_test_acc_{test_acc:.4f}_val_acc_{max_val_acc_this_run:.4f}.keras"
filepath_to_save = os.path.join(model_save_dir, model_filename)
print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {filepath_to_save} ...")
model.save(filepath_to_save)
print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
print("-" * 50)

model_paths = [
    "saved_ensemble_models/ensemble_model_S3_test_acc_0.6042_val_acc_0.5798.keras",
    "saved_ensemble_models/ensemble_model_S5_test_acc_0.6003_val_acc_0.5854.keras"
]

all_probas = []

for model_path in model_paths:
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª—å—é: {model_path}")
    model = tf.keras.models.load_model(model_path)
    probas = model.predict(X_test, verbose=0).flatten()
    all_probas.append(probas)
    tf.keras.backend.clear_session()

stacked_probas = np.stack(all_probas, axis=0)
mean_probas = np.mean(stacked_probas, axis=0)

ensemble_preds = (mean_probas > 0.5).astype(int)

ensemble_accuracy = accuracy_score(y_test, ensemble_preds)
print(f"\nEnsemble Test Accuracy (average probability, {len(model_paths)} models): {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)")

confident_ensemble_predictions = np.sum((mean_probas > 0.7) | (mean_probas < 0.3))
print(f"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {confident_ensemble_predictions}/{len(mean_probas)} ({confident_ensemble_predictions/len(mean_probas)*100:.1f}%)")

"""# Upload real data

## Extracting Features from JSON
"""

df_real_test = pd.read_csv("fot_test_500.csv")
df_real_test.drop(columns=[col for col in df_real_test.columns if 'Unnamed:' in col], inplace=True, errors='ignore')

price_cols = ['outcome_price', 'initial_entry_price']
all_potential_feature_cols = [col for col in df_real_test.columns if col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value']

if 'feat_market_state' in df_real_test.columns and df_real_test['feat_market_state'].dtype == 'object':
    df_real_test['feat_market_state'] = df_real_test['feat_market_state'].map({'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}).fillna(0)

cols_to_convert_numeric = price_cols + all_potential_feature_cols
for col_name in list(set(cols_to_convert_numeric)):
    if col_name in df_real_test.columns:
        df_real_test[col_name] = pd.to_numeric(df_real_test[col_name].astype(str).str.replace(',', '.'), errors='coerce')

cols_for_dropna_real = price_cols[:]
if 'outcome_status' in df_real_test.columns: cols_for_dropna_real.append('outcome_status')
df_real_test.dropna(subset=cols_for_dropna_real, inplace=True)
if df_real_test.empty: raise ValueError("–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–π –æ—á–∏—Å—Ç–∫–∏.")

base_feature_columns_real = [col for col in all_potential_feature_cols if col in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test[col])]
if 'feat_market_state' in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test['feat_market_state']) and 'feat_market_state' not in base_feature_columns_real:
    base_feature_columns_real.append('feat_market_state')
base_feature_columns_real = list(set(base_feature_columns_real))
df_real_test[base_feature_columns_real] = df_real_test[base_feature_columns_real].fillna(0)

new_cols_data_real = {}
lags = [1, 2, 4, 8]; roll_windows = [4, 8]
for col in base_feature_columns_real:
    for lag in lags: new_cols_data_real[f'{col}_lag{lag}'] = df_real_test[col].shift(lag)
    for window in roll_windows: new_cols_data_real[f'{col}_roll_mean{window}'] = df_real_test[col].rolling(window=window, min_periods=1).mean()
df_real_test = pd.concat([df_real_test, pd.DataFrame(new_cols_data_real, index=df_real_test.index)], axis=1)
df_real_test.dropna(inplace=True)
if df_real_test.empty: raise ValueError("–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ Feature Engineering.")

for col_to_check in X_before_selection_columns: # X_before_selection_columns –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–∞–Ω–µ–µ!
    if col_to_check not in df_real_test.columns: df_real_test[col_to_check] = 0
X_real_test_prepared = df_real_test[X_before_selection_columns].fillna(0) # fillna(0) –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫

X_real_test_scaled_all = scaler.transform(X_real_test_prepared)
X_real_test_scaled_all = np.nan_to_num(X_real_test_scaled_all, nan=0.0, posinf=0.0, neginf=0.0)
X_real_test_selected = selector.transform(X_real_test_scaled_all)

y_real_test = (df_real_test.loc[X_real_test_prepared.index, 'outcome_status'] == 'LONG_MET').astype(int) if 'outcome_status' in df_real_test.columns and not df_real_test.empty else None

model_ensemble_paths = [
    "saved_ensemble_models/ensemble_model_S10_test_acc_0.5996_val_acc_0.5752.keras",
    "saved_ensemble_models/ensemble_model_S7_test_acc_0.5918_val_acc_0.5798.keras",
]
all_real_probas = []
for model_path in model_ensemble_paths:
    if not os.path.exists(model_path): print(f"–ü—Ä–æ–ø—É—Å–∫: {model_path}"); continue
    model = tf.keras.models.load_model(model_path)
    all_real_probas.append(model.predict(X_real_test_selected, verbose=0).flatten())
    tf.keras.backend.clear_session()

if not all_real_probas: raise ValueError("–ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

mean_real_probas = np.mean(np.stack(all_real_probas, axis=0), axis=0)
ensemble_real_preds = (mean_real_probas > 0.5).astype(int)

print(f"\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ({len(X_real_test_selected)} –∑–∞–ø–∏—Å–µ–π) ---")
for i in range(min(5, len(X_real_test_selected))):
    pred_label = "LONG" if ensemble_real_preds[i] == 1 else "SHORT"
    actual_val = y_real_test.iloc[i] if y_real_test is not None and i < len(y_real_test) else "N/A"
    actual_label_info = f"(–†–µ–∞–ª—å–Ω—ã–π: {actual_val})" if y_real_test is not None else ""
    print(f"–ü—Ä–∏–º–µ—Ä {i+1}: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å LONG={mean_real_probas[i]:.4f}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ={pred_label} {actual_label_info}")

if y_real_test is not None and len(y_real_test) == len(ensemble_real_preds):
    ensemble_real_accuracy = accuracy_score(y_real_test, ensemble_real_preds)
    print(f"\nEnsemble Accuracy –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {ensemble_real_accuracy:.4f} ({ensemble_real_accuracy*100:.2f}%)")
    print("\nClassification Report –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(classification_report(y_real_test, ensemble_real_preds, target_names=['SHORT', 'LONG'], zero_division=0))

    confident_ensemble_predictions = np.sum((mean_real_probas > 0.7) | (mean_real_probas < 0.3))
    print(f"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è: {confident_ensemble_predictions}/{len(mean_real_probas)} ({confident_ensemble_predictions/len(mean_real_probas)*100:.1f}%)")
else:
    print("\n–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å: y_real_test –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç.")

"""## Predict Results"""

results = []

def convert_numpy_types(obj):
    if isinstance(obj, (np.generic,)):
        return obj.item()
    if isinstance(obj, np.ndarray): # Handle ndarray conversion
        return obj.tolist()
    if obj == np.inf or obj == -np.inf:
        return "Infinity" # Or None, or a large number string
    if np.isnan(obj):
        return None # Or "NaN"
    return obj

def format_symbol(symbol):
    if isinstance(symbol, str):
        return symbol.replace("/USDT", "").upper()
    return symbol

# Corrected print statements for X
if isinstance(X, np.ndarray):
    print(f"–§–æ—Ä–º–∞ X (NumPy array): {X.shape}")
    if X.ndim > 1:
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –≤ X: {X.shape[1]}")
    elif X.ndim == 1 and len(df) == X.shape[0]: # If X is 1D but per row for df (unlikely for features)
        print(f"X is 1D, length: {X.shape[0]}")
    elif X.ndim == 1: # If X is a single feature vector not matching df rows
         print(f"X is a single 1D feature vector, length: {X.shape[0]}")

else: # Should not happen based on the error, but good for robustness
    print("X is not a NumPy array, attempting to use .columns")
    print("–ö–æ–ª–æ–Ω–∫–∏ –≤ X:", list(X.columns))
    print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –≤ X:", len(X.columns))


# Assuming X and df have the same number of rows
# and X contains the features for each row in df.
if len(df) != X.shape[0] and isinstance(X, np.ndarray):
    print(f"–í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ df ({len(df)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–æ–∫ –≤ X ({X.shape[0]})!")
    # Decide how to proceed or stop if this is an issue

for i, row in df.iterrows():
    try:
        # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ X –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏
        # X[i] will give the i-th row if X is a 2D NumPy array
        current_row_features = X[i].reshape(1, -1)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        # It's assumed 'scaler_X' is fitted on data with the same structure as rows of X
        model_input_scaled = scaler_X.transform(current_row_features)
        pred_percent_change = model.predict(model_input_scaled, verbose=0).flatten()[0]

        # –û–±—Ä–µ–∑–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        if pred_percent_change < -50:
            pred_percent_change = -10.0
        elif pred_percent_change > 50:
            pred_percent_change = 10.0

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        initial_price = float(row.get('initial_entry_price', 0))
        actual_price = float(row.get('outcome_price', 0))
        actual_percent_change = ((actual_price - initial_price) / initial_price * 100) if initial_price != 0 else 0


        result = {
            "symbol": format_symbol(row.get("symbol", "UNKNOWN")),
            "predicted_change_percent": float(pred_percent_change),
            "actual_change_percent": float(actual_percent_change),
            "actual_price": float(actual_price),
            "initial_price": float(initial_price),
        }
        results.append(result)

        if (i + 1) % 100 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(df)}")

    except IndexError as e:
        print(f"–û—à–∏–±–∫–∞ IndexError –≤ —Å—Ç—Ä–æ–∫–µ {i} (–≤–æ–∑–º–æ–∂–Ω–æ, X –∫–æ—Ä–æ—á–µ, —á–µ–º df): {e}")
        print(f"–î–ª–∏–Ω–∞ df: {len(df)}, –§–æ—Ä–º–∞ X: {X.shape if isinstance(X, np.ndarray) else '–ù–µ NumPy –º–∞—Å—Å–∏–≤'}")
        continue # Or break, depending on how critical this is
    except Exception as e:
        print(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}: {e}")
        continue

# –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
valid_results = []
for result in results:
    pred = result['predicted_change_percent']
    if pred is not None and not np.isnan(pred) and not np.isinf(pred):
        valid_results.append(result)

print(f"–í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(valid_results)} –∏–∑ {len(results)}")

if valid_results:
    predicted_changes = [r['predicted_change_percent'] for r in valid_results]
    actual_changes = [r['actual_change_percent'] for r in valid_results]

    # Filter out potential non-numeric or problematic actual_price values before finding min/max
    valid_actual_prices = [r['actual_price'] for r in valid_results if isinstance(r['actual_price'], (int, float)) and r['actual_price'] > 0]

    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {min(predicted_changes):.2f}% - {max(predicted_changes):.2f}%")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {np.mean(predicted_changes):.2f}%")
    if actual_changes: # Ensure actual_changes is not empty
        print(f"–î–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {min(actual_changes):.2f}% - {max(actual_changes):.2f}%")
    else:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.")

    if valid_actual_prices:
        print(f"–î–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {min(valid_actual_prices):.4f} - {max(valid_actual_prices):.4f}")
    else:
        print("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞.")


# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
output_data = {
    "total_predictions": len(valid_results),
    "predictions": valid_results
}

with open("final_predictions.json", 'w', encoding='utf-8') as f:
    json.dump(output_data, f, indent=2, ensure_ascii=False, default=convert_numpy_types)

print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(valid_results)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ final_predictions.json")