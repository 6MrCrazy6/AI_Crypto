{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOw8yMd1VlnD"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvUGC8QQV6bV"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wfFEXZC0WS-V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from keras import regularizers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhYaZ-ENV_c5"
      },
      "source": [
        "## Importing the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aqHTg9bxWT_u"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"new_dataset_15k.csv\", sep=';', decimal=',', encoding='cp1251')\n",
        "df.drop(columns=[col for col in df.columns if 'Unnamed:' in col], inplace=True, errors='ignore')\n",
        "\n",
        "price_cols = ['outcome_price', 'initial_entry_price']\n",
        "all_potential_feature_cols = [\n",
        "    col for col in df.columns if\n",
        "    col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value'\n",
        "]\n",
        "\n",
        "if 'feat_market_state' in df.columns and df['feat_market_state'].dtype == 'object':\n",
        "    df['feat_market_state'] = df['feat_market_state'].map(\n",
        "        {'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}\n",
        "    ).fillna(0)\n",
        "\n",
        "cols_to_convert_numeric = price_cols + all_potential_feature_cols\n",
        "for col_name in list(set(cols_to_convert_numeric)):\n",
        "    if col_name in df.columns:\n",
        "        df[col_name] = pd.to_numeric(\n",
        "            df[col_name].astype(str).str.replace(',', '.'), errors='coerce'\n",
        "        )\n",
        "\n",
        "df.dropna(subset=price_cols + ['outcome_status'], inplace=True)\n",
        "if df.empty:\n",
        "    raise ValueError(\"DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN –≤ —Ü–µ–Ω–∞—Ö/—Å—Ç–∞—Ç—É—Å–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering"
      ],
      "metadata": {
        "id": "5CqAm82dqGsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_feature_columns = [\n",
        "    col for col in all_potential_feature_cols\n",
        "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col])\n",
        "]\n",
        "if 'feat_market_state' in df.columns and \\\n",
        "   pd.api.types.is_numeric_dtype(df['feat_market_state']) and \\\n",
        "   'feat_market_state' not in base_feature_columns:\n",
        "    base_feature_columns.append('feat_market_state')\n",
        "base_feature_columns = list(set(base_feature_columns))\n",
        "\n",
        "df[base_feature_columns] = df[base_feature_columns].fillna(0)\n",
        "\n",
        "engineered_feature_columns = base_feature_columns.copy()\n",
        "lags = [1, 2, 4, 8]\n",
        "roll_windows = [4, 8]\n",
        "new_cols_data = {}\n",
        "\n",
        "for col in base_feature_columns:\n",
        "    for lag in lags:\n",
        "        new_cols_data[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
        "    for window in roll_windows:\n",
        "        new_cols_data[f'{col}_roll_mean{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "df = pd.concat([df, pd.DataFrame(new_cols_data, index=df.index)], axis=1)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ Feature Engineering –∏ dropna. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∞–≥–∏/–æ–∫–Ω–∞.\")\n",
        "\n",
        "feature_columns = list(set(engineered_feature_columns + list(new_cols_data.keys())))"
      ],
      "metadata": {
        "id": "bNZ76En4qIuR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formation of a sample of \"strong movements\" and final X, y"
      ],
      "metadata": {
        "id": "tuU66aguqVQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['price_change'] = (df['outcome_price'] - df['initial_entry_price']) / df['initial_entry_price'] * 100\n",
        "strong_mask = (((df['outcome_status'] == 'LONG_MET') & (df['price_change'] > 0.5)) |\n",
        "               ((df['outcome_status'] == 'SHORT_MET') & (df['price_change'] < -0.5)))\n",
        "strong_moves = df.loc[strong_mask].copy()\n",
        "\n",
        "if strong_moves.empty:\n",
        "    raise ValueError(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ 'strong_moves'.\")\n",
        "\n",
        "actual_feature_cols_in_strong = [col for col in feature_columns if col in strong_moves.columns]\n",
        "X_prepared = strong_moves[actual_feature_cols_in_strong].copy()\n",
        "X_prepared.fillna(0, inplace=True)\n",
        "\n",
        "valid_cols = [col for col in actual_feature_cols_in_strong if X_prepared[col].std() > 1e-6]\n",
        "if not valid_cols:\n",
        "    raise ValueError(\"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö (–Ω–µ–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö) –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\")\n",
        "\n",
        "X_before_selection = X_prepared[valid_cols].copy()\n",
        "y_labels = (strong_moves['outcome_status'] == 'LONG_MET').astype(int)\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä—ã –ø–µ—Ä–µ–¥ –æ—Ç–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: X={X_before_selection.shape}, y={y_labels.shape}\")"
      ],
      "metadata": {
        "id": "TNEBULgtqWsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abSxRqvWEIB"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm48sif-WWsh"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled_all_features = scaler.fit_transform(X_before_selection)\n",
        "X_scaled_all_features = np.nan_to_num(X_scaled_all_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "NUM_FEATURES_TO_SELECT = 80\n",
        "print(f\"–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {X_scaled_all_features.shape[1]} –¥–æ {NUM_FEATURES_TO_SELECT}...\")\n",
        "\n",
        "selector = SelectFromModel(\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    max_features=NUM_FEATURES_TO_SELECT,\n",
        "    threshold=-np.inf\n",
        ")\n",
        "\n",
        "selector.fit(X_scaled_all_features, y_labels)\n",
        "X_selected_features = selector.transform(X_scaled_all_features)\n",
        "\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X_before_selection.columns[selected_feature_indices].tolist()\n",
        "\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞: {X_selected_features.shape[1]}\")\n",
        "if 0 < X_selected_features.shape[1] <= 10: # –ü–æ–∫–∞–∑–∞—Ç—å –∏–º–µ–Ω–∞, –µ—Å–ª–∏ –∏—Ö –Ω–µ–º–Ω–æ–≥–æ\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {selected_feature_names[:min(10, len(selected_feature_names))]}\")\n",
        "elif X_selected_features.shape[1] > 10:\n",
        "     print(f\"–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 5 –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {selected_feature_names[:5]}\")\n",
        "\n",
        "\n",
        "if X_selected_features.shape[1] == 0:\n",
        "    raise ValueError(\"–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–≤–∏–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã SelectFromModel.\")\n",
        "if X_selected_features.shape[1] != NUM_FEATURES_TO_SELECT:\n",
        "    print(f\"–í–ù–ò–ú–ê–ù–ò–ï: –û—Ç–æ–±—Ä–∞–Ω–æ {X_selected_features.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ö–æ—Ç—è –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–æ—Å—å {NUM_FEATURES_TO_SELECT}. \"\n",
        "          \"–≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –µ—Å–ª–∏ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–Ω—å—à–µ –∏–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é.\")\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_selected_features, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä—ã –∏—Ç–æ–≥–æ–≤—ã—Ö –≤—ã–±–æ—Ä–æ–∫:\")\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"y_train: 0={np.sum(y_train==0)}, 1={np.sum(y_train==1)}\")\n",
        "print(f\"y_val:   0={np.sum(y_val==0)}, 1={np.sum(y_val==1)}\")\n",
        "print(f\"y_test:  0={np.sum(y_test==0)}, 1={np.sum(y_test==1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrNFWUgGAbi2"
      },
      "source": [
        "# AI Create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_yMBE-oA37z"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkmiaQkwA3XH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "MODEL_SEED = 10\n",
        "np.random.seed(MODEL_SEED)\n",
        "tf.random.set_seed(MODEL_SEED)\n",
        "print(f\"–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ê–ù–°–ê–ú–ë–õ–Ø –° MODEL_SEED = {MODEL_SEED}\")\n",
        "# -------------------------------------------------------\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.35),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.35),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(32, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(16, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "class_weight = {0: 1.0, 1: 2.5}\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_save_dir = \"saved_ensemble_models\"\n",
        "if not os.path.exists(model_save_dir):\n",
        "    os.makedirs(model_save_dir)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=90,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=30,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–Ω—Å–∞–º–±–ª—è (SEED={MODEL_SEED})...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "max_val_acc_this_run = 0\n",
        "if hasattr(history, 'history') and history.history and 'val_accuracy' in history.history:\n",
        "    max_val_acc_this_run = max(history.history['val_accuracy'])\n",
        "print(f\"Max val_accuracy –¥–ª—è SEED={MODEL_SEED}: {max_val_acc_this_run:.4f}\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy –¥–ª—è SEED={MODEL_SEED}: {test_acc:.4f}\")\n",
        "\n",
        "test_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
        "confident_predictions = np.sum((test_pred_proba > 0.7) | (test_pred_proba < 0.3))\n",
        "print(f\"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ: {confident_predictions}/{len(test_pred_proba)} ({confident_predictions/len(test_pred_proba)*100:.1f}%)\")\n",
        "\n",
        "model_filename = f\"ensemble_model_S{MODEL_SEED}_test_acc_{test_acc:.4f}_val_acc_{max_val_acc_this_run:.4f}.keras\"\n",
        "filepath_to_save = os.path.join(model_save_dir, model_filename)\n",
        "print(f\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {filepath_to_save} ...\")\n",
        "model.save(filepath_to_save)\n",
        "print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = [\n",
        "    \"saved_ensemble_models/ensemble_model_S3_test_acc_0.6042_val_acc_0.5798.keras\",\n",
        "    \"saved_ensemble_models/ensemble_model_S5_test_acc_0.6003_val_acc_0.5854.keras\"\n",
        "]\n",
        "\n",
        "all_probas = []\n",
        "\n",
        "for model_path in model_paths:\n",
        "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª—å—é: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    probas = model.predict(X_test, verbose=0).flatten()\n",
        "    all_probas.append(probas)\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "stacked_probas = np.stack(all_probas, axis=0)\n",
        "mean_probas = np.mean(stacked_probas, axis=0)\n",
        "\n",
        "ensemble_preds = (mean_probas > 0.5).astype(int)\n",
        "\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f\"\\nEnsemble Test Accuracy (average probability, {len(model_paths)} models): {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
        "\n",
        "confident_ensemble_predictions = np.sum((mean_probas > 0.7) | (mean_probas < 0.3))\n",
        "print(f\"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {confident_ensemble_predictions}/{len(mean_probas)} ({confident_ensemble_predictions/len(mean_probas)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "f3NJgSACXgzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5s6HmNlOQNU"
      },
      "source": [
        "# Upload real data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAGwL4G9n3S"
      },
      "source": [
        "## Extracting Features from JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFBedrTy9qRD"
      },
      "outputs": [],
      "source": [
        "df_real_test = pd.read_csv(\"fot_test_500.csv\")\n",
        "df_real_test.drop(columns=[col for col in df_real_test.columns if 'Unnamed:' in col], inplace=True, errors='ignore')\n",
        "\n",
        "price_cols = ['outcome_price', 'initial_entry_price']\n",
        "all_potential_feature_cols = [col for col in df_real_test.columns if col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value']\n",
        "\n",
        "if 'feat_market_state' in df_real_test.columns and df_real_test['feat_market_state'].dtype == 'object':\n",
        "    df_real_test['feat_market_state'] = df_real_test['feat_market_state'].map({'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}).fillna(0)\n",
        "\n",
        "cols_to_convert_numeric = price_cols + all_potential_feature_cols\n",
        "for col_name in list(set(cols_to_convert_numeric)):\n",
        "    if col_name in df_real_test.columns:\n",
        "        df_real_test[col_name] = pd.to_numeric(df_real_test[col_name].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "\n",
        "cols_for_dropna_real = price_cols[:]\n",
        "if 'outcome_status' in df_real_test.columns: cols_for_dropna_real.append('outcome_status')\n",
        "df_real_test.dropna(subset=cols_for_dropna_real, inplace=True)\n",
        "if df_real_test.empty: raise ValueError(\"–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–π –æ—á–∏—Å—Ç–∫–∏.\")\n",
        "\n",
        "base_feature_columns_real = [col for col in all_potential_feature_cols if col in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test[col])]\n",
        "if 'feat_market_state' in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test['feat_market_state']) and 'feat_market_state' not in base_feature_columns_real:\n",
        "    base_feature_columns_real.append('feat_market_state')\n",
        "base_feature_columns_real = list(set(base_feature_columns_real))\n",
        "df_real_test[base_feature_columns_real] = df_real_test[base_feature_columns_real].fillna(0)\n",
        "\n",
        "new_cols_data_real = {}\n",
        "lags = [1, 2, 4, 8]; roll_windows = [4, 8]\n",
        "for col in base_feature_columns_real:\n",
        "    for lag in lags: new_cols_data_real[f'{col}_lag{lag}'] = df_real_test[col].shift(lag)\n",
        "    for window in roll_windows: new_cols_data_real[f'{col}_roll_mean{window}'] = df_real_test[col].rolling(window=window, min_periods=1).mean()\n",
        "df_real_test = pd.concat([df_real_test, pd.DataFrame(new_cols_data_real, index=df_real_test.index)], axis=1)\n",
        "df_real_test.dropna(inplace=True)\n",
        "if df_real_test.empty: raise ValueError(\"–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame –ø—É—Å—Ç –ø–æ—Å–ª–µ Feature Engineering.\")\n",
        "\n",
        "for col_to_check in X_before_selection_columns: # X_before_selection_columns –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–∞–Ω–µ–µ!\n",
        "    if col_to_check not in df_real_test.columns: df_real_test[col_to_check] = 0\n",
        "X_real_test_prepared = df_real_test[X_before_selection_columns].fillna(0) # fillna(0) –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "\n",
        "# --- –®–∞–≥ 2: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\n",
        "# scaler –∏ selector –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤–∞—à–∏–º–∏ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏!\n",
        "X_real_test_scaled_all = scaler.transform(X_real_test_prepared)\n",
        "X_real_test_scaled_all = np.nan_to_num(X_real_test_scaled_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_real_test_selected = selector.transform(X_real_test_scaled_all)\n",
        "\n",
        "y_real_test = (df_real_test.loc[X_real_test_prepared.index, 'outcome_status'] == 'LONG_MET').astype(int) if 'outcome_status' in df_real_test.columns and not df_real_test.empty else None\n",
        "\n",
        "# --- –®–∞–≥ 3: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–º ---\n",
        "model_ensemble_paths = [ # –û–±–Ω–æ–≤–∏—Ç–µ —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ –ø—É—Ç—è–º–∏ –∫ –≤–∞—à–∏–º –ª—É—á—à–∏–º –º–æ–¥–µ–ª—è–º\n",
        "    \"saved_ensemble_models/ensemble_model_S10_test_acc_0.5996_val_acc_0.5752.keras\",\n",
        "    \"saved_ensemble_models/ensemble_model_S7_test_acc_0.5918_val_acc_0.5798.keras\",\n",
        "]\n",
        "all_real_probas = []\n",
        "for model_path in model_ensemble_paths:\n",
        "    if not os.path.exists(model_path): print(f\"–ü—Ä–æ–ø—É—Å–∫: {model_path}\"); continue\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    all_real_probas.append(model.predict(X_real_test_selected, verbose=0).flatten())\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "if not all_real_probas: raise ValueError(\"–ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")\n",
        "\n",
        "mean_real_probas = np.mean(np.stack(all_real_probas, axis=0), axis=0)\n",
        "ensemble_real_preds = (mean_real_probas > 0.5).astype(int)\n",
        "\n",
        "print(f\"\\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ({len(X_real_test_selected)} –∑–∞–ø–∏—Å–µ–π) ---\")\n",
        "for i in range(min(5, len(X_real_test_selected))):\n",
        "    pred_label = \"LONG\" if ensemble_real_preds[i] == 1 else \"SHORT\"\n",
        "    actual_val = y_real_test.iloc[i] if y_real_test is not None and i < len(y_real_test) else \"N/A\"\n",
        "    actual_label_info = f\"(–†–µ–∞–ª—å–Ω—ã–π: {actual_val})\" if y_real_test is not None else \"\"\n",
        "    print(f\"–ü—Ä–∏–º–µ—Ä {i+1}: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å LONG={mean_real_probas[i]:.4f}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ={pred_label} {actual_label_info}\")\n",
        "\n",
        "if y_real_test is not None and len(y_real_test) == len(ensemble_real_preds):\n",
        "    ensemble_real_accuracy = accuracy_score(y_real_test, ensemble_real_preds)\n",
        "    print(f\"\\nEnsemble Accuracy –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {ensemble_real_accuracy:.4f} ({ensemble_real_accuracy*100:.2f}%)\")\n",
        "    print(\"\\nClassification Report –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(classification_report(y_real_test, ensemble_real_preds, target_names=['SHORT', 'LONG'], zero_division=0))\n",
        "\n",
        "    confident_ensemble_predictions = np.sum((mean_real_probas > 0.7) | (mean_real_probas < 0.3))\n",
        "    print(f\"–£–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è: {confident_ensemble_predictions}/{len(mean_real_probas)} ({confident_ensemble_predictions/len(mean_real_probas)*100:.1f}%)\")\n",
        "else:\n",
        "    print(\"\\n–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å: y_real_test –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD8-Hrn798Bb"
      },
      "source": [
        "## Predict Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHEBi2iY9-zM"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, (np.generic,)):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, np.ndarray): # Handle ndarray conversion\n",
        "        return obj.tolist()\n",
        "    if obj == np.inf or obj == -np.inf:\n",
        "        return \"Infinity\" # Or None, or a large number string\n",
        "    if np.isnan(obj):\n",
        "        return None # Or \"NaN\"\n",
        "    return obj\n",
        "\n",
        "def format_symbol(symbol):\n",
        "    if isinstance(symbol, str):\n",
        "        return symbol.replace(\"/USDT\", \"\").upper()\n",
        "    return symbol\n",
        "\n",
        "# Corrected print statements for X\n",
        "if isinstance(X, np.ndarray):\n",
        "    print(f\"–§–æ—Ä–º–∞ X (NumPy array): {X.shape}\")\n",
        "    if X.ndim > 1:\n",
        "        print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ (–ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –≤ X: {X.shape[1]}\")\n",
        "    elif X.ndim == 1 and len(df) == X.shape[0]: # If X is 1D but per row for df (unlikely for features)\n",
        "        print(f\"X is 1D, length: {X.shape[0]}\")\n",
        "    elif X.ndim == 1: # If X is a single feature vector not matching df rows\n",
        "         print(f\"X is a single 1D feature vector, length: {X.shape[0]}\")\n",
        "\n",
        "else: # Should not happen based on the error, but good for robustness\n",
        "    print(\"X is not a NumPy array, attempting to use .columns\")\n",
        "    print(\"–ö–æ–ª–æ–Ω–∫–∏ –≤ X:\", list(X.columns))\n",
        "    print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –≤ X:\", len(X.columns))\n",
        "\n",
        "\n",
        "# Assuming X and df have the same number of rows\n",
        "# and X contains the features for each row in df.\n",
        "if len(df) != X.shape[0] and isinstance(X, np.ndarray):\n",
        "    print(f\"–í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ df ({len(df)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–æ–∫ –≤ X ({X.shape[0]})!\")\n",
        "    # Decide how to proceed or stop if this is an issue\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ X –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–∏\n",
        "        # X[i] will give the i-th row if X is a 2D NumPy array\n",
        "        current_row_features = X[i].reshape(1, -1)\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è\n",
        "        # It's assumed 'scaler_X' is fitted on data with the same structure as rows of X\n",
        "        model_input_scaled = scaler_X.transform(current_row_features)\n",
        "        pred_percent_change = model.predict(model_input_scaled, verbose=0).flatten()[0]\n",
        "\n",
        "        # –û–±—Ä–µ–∑–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è\n",
        "        if pred_percent_change < -50:\n",
        "            pred_percent_change = -10.0\n",
        "        elif pred_percent_change > 50:\n",
        "            pred_percent_change = 10.0\n",
        "\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "        initial_price = float(row.get('initial_entry_price', 0))\n",
        "        actual_price = float(row.get('outcome_price', 0))\n",
        "        actual_percent_change = ((actual_price - initial_price) / initial_price * 100) if initial_price != 0 else 0\n",
        "\n",
        "\n",
        "        result = {\n",
        "            \"symbol\": format_symbol(row.get(\"symbol\", \"UNKNOWN\")),\n",
        "            \"predicted_change_percent\": float(pred_percent_change),\n",
        "            \"actual_change_percent\": float(actual_percent_change),\n",
        "            \"actual_price\": float(actual_price),\n",
        "            \"initial_price\": float(initial_price),\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(df)}\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ IndexError –≤ —Å—Ç—Ä–æ–∫–µ {i} (–≤–æ–∑–º–æ–∂–Ω–æ, X –∫–æ—Ä–æ—á–µ, —á–µ–º df): {e}\")\n",
        "        print(f\"–î–ª–∏–Ω–∞ df: {len(df)}, –§–æ—Ä–º–∞ X: {X.shape if isinstance(X, np.ndarray) else '–ù–µ NumPy –º–∞—Å—Å–∏–≤'}\")\n",
        "        continue # Or break, depending on how critical this is\n",
        "    except Exception as e:\n",
        "        print(f\"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "# –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "valid_results = []\n",
        "for result in results:\n",
        "    pred = result['predicted_change_percent']\n",
        "    if pred is not None and not np.isnan(pred) and not np.isinf(pred):\n",
        "        valid_results.append(result)\n",
        "\n",
        "print(f\"–í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(valid_results)} –∏–∑ {len(results)}\")\n",
        "\n",
        "if valid_results:\n",
        "    predicted_changes = [r['predicted_change_percent'] for r in valid_results]\n",
        "    actual_changes = [r['actual_change_percent'] for r in valid_results]\n",
        "\n",
        "    # Filter out potential non-numeric or problematic actual_price values before finding min/max\n",
        "    valid_actual_prices = [r['actual_price'] for r in valid_results if isinstance(r['actual_price'], (int, float)) and r['actual_price'] > 0]\n",
        "\n",
        "    print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {min(predicted_changes):.2f}% - {max(predicted_changes):.2f}%\")\n",
        "    print(f\"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {np.mean(predicted_changes):.2f}%\")\n",
        "    if actual_changes: # Ensure actual_changes is not empty\n",
        "        print(f\"–î–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {min(actual_changes):.2f}% - {max(actual_changes):.2f}%\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\")\n",
        "\n",
        "    if valid_actual_prices:\n",
        "        print(f\"–î–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {min(valid_actual_prices):.4f} - {max(valid_actual_prices):.4f}\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞.\")\n",
        "\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "output_data = {\n",
        "    \"total_predictions\": len(valid_results),\n",
        "    \"predictions\": valid_results\n",
        "}\n",
        "\n",
        "with open(\"final_predictions.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False, default=convert_numpy_types)\n",
        "\n",
        "print(f\"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(valid_results)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ final_predictions.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WOw8yMd1VlnD",
        "NvUGC8QQV6bV",
        "fhYaZ-ENV_c5",
        "3abSxRqvWEIB",
        "MrNFWUgGAbi2",
        "L5s6HmNlOQNU",
        "RyAGwL4G9n3S"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}