{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/6MrCrazy6/AI_Crypto/blob/main/AI_Crypto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOw8yMd1VlnD"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvUGC8QQV6bV"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wfFEXZC0WS-V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from keras import regularizers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhYaZ-ENV_c5"
      },
      "source": [
        "## Importing the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aqHTg9bxWT_u"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"new_dataset_15k.csv\", sep=';', decimal=',', encoding='cp1251')\n",
        "df.drop(columns=[col for col in df.columns if 'Unnamed:' in col], inplace=True, errors='ignore')\n",
        "\n",
        "price_cols = ['outcome_price', 'initial_entry_price']\n",
        "all_potential_feature_cols = [\n",
        "    col for col in df.columns if\n",
        "    col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value'\n",
        "]\n",
        "\n",
        "if 'feat_market_state' in df.columns and df['feat_market_state'].dtype == 'object':\n",
        "    df['feat_market_state'] = df['feat_market_state'].map(\n",
        "        {'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}\n",
        "    ).fillna(0)\n",
        "\n",
        "cols_to_convert_numeric = price_cols + all_potential_feature_cols\n",
        "for col_name in list(set(cols_to_convert_numeric)):\n",
        "    if col_name in df.columns:\n",
        "        df[col_name] = pd.to_numeric(\n",
        "            df[col_name].astype(str).str.replace(',', '.'), errors='coerce'\n",
        "        )\n",
        "\n",
        "df.dropna(subset=price_cols + ['outcome_status'], inplace=True)\n",
        "if df.empty:\n",
        "    raise ValueError(\"DataFrame Ğ¿ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ NaN Ğ² Ñ†ĞµĞ½Ğ°Ñ…/ÑÑ‚Ğ°Ñ‚ÑƒÑĞµ. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering"
      ],
      "metadata": {
        "id": "5CqAm82dqGsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_feature_columns = [\n",
        "    col for col in all_potential_feature_cols\n",
        "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col])\n",
        "]\n",
        "if 'feat_market_state' in df.columns and \\\n",
        "   pd.api.types.is_numeric_dtype(df['feat_market_state']) and \\\n",
        "   'feat_market_state' not in base_feature_columns:\n",
        "    base_feature_columns.append('feat_market_state')\n",
        "base_feature_columns = list(set(base_feature_columns))\n",
        "\n",
        "df[base_feature_columns] = df[base_feature_columns].fillna(0)\n",
        "\n",
        "engineered_feature_columns = base_feature_columns.copy()\n",
        "lags = [1, 2, 4, 8]\n",
        "roll_windows = [4, 8]\n",
        "new_cols_data = {}\n",
        "\n",
        "for col in base_feature_columns:\n",
        "    for lag in lags:\n",
        "        new_cols_data[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
        "    for window in roll_windows:\n",
        "        new_cols_data[f'{col}_roll_mean{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "df = pd.concat([df, pd.DataFrame(new_cols_data, index=df.index)], axis=1)\n",
        "df.dropna(inplace=True) # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ NaN, Ğ¿Ğ¾ÑĞ²Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ¸Ğ·-Ğ·Ğ° shift/rolling\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"DataFrame Ğ¿ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Feature Engineering Ğ¸ dropna. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ»Ğ°Ğ³Ğ¸/Ğ¾ĞºĞ½Ğ°.\")\n",
        "\n",
        "feature_columns = list(set(engineered_feature_columns + list(new_cols_data.keys()))) # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²"
      ],
      "metadata": {
        "id": "bNZ76En4qIuR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ \"ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹\" Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… X, y"
      ],
      "metadata": {
        "id": "tuU66aguqVQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ df Ğ¸ feature_columns ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ· Ğ‘Ğ»Ğ¾ĞºĞ° 1 Ğ¸ 2) ...\n",
        "\n",
        "df['price_change'] = (df['outcome_price'] - df['initial_entry_price']) / df['initial_entry_price'] * 100\n",
        "strong_mask = (((df['outcome_status'] == 'LONG_MET') & (df['price_change'] > 0.5)) |\n",
        "               ((df['outcome_status'] == 'SHORT_MET') & (df['price_change'] < -0.5)))\n",
        "strong_moves = df.loc[strong_mask].copy()\n",
        "\n",
        "if strong_moves.empty:\n",
        "    raise ValueError(\"ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ 'strong_moves'.\")\n",
        "\n",
        "actual_feature_cols_in_strong = [col for col in feature_columns if col in strong_moves.columns]\n",
        "X_prepared = strong_moves[actual_feature_cols_in_strong].copy()\n",
        "X_prepared.fillna(0, inplace=True)\n",
        "\n",
        "valid_cols = [col for col in actual_feature_cols_in_strong if X_prepared[col].std() > 1e-6]\n",
        "if not valid_cols:\n",
        "    raise ValueError(\"ĞĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… (Ğ½ĞµĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ…) ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ².\")\n",
        "\n",
        "# Ğ’Ğ¾Ñ‚ Ğ·Ğ´ĞµÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ X_before_selection Ğ¸ y_labels\n",
        "X_before_selection = X_prepared[valid_cols].copy()\n",
        "y_labels = (strong_moves['outcome_status'] == 'LONG_MET').astype(int)\n",
        "\n",
        "print(f\"Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: X={X_before_selection.shape}, y={y_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNEBULgtqWsk",
        "outputId": "189eae3f-9e58-4c16-ddbd-916e66da1545"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: X=(5101, 259), y=(5101,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abSxRqvWEIB"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hm48sif-WWsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32f1678-c63c-4c9c-d337-fc1634ff1a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ĞÑ‚Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 259 Ğ´Ğ¾ 80...\n",
            "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ°: 80\n",
            "ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… 5 Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: ['feat_price_pct_change_roll_mean4', 'feat_btc_correlation_lag2', 'feat_rsi_roll_mean8', 'feat_spread_pct_lag8', 'feat_long_liq_usd_roll_mean8']\n",
            "Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº:\n",
            "Train: (2499, 80), Val: (1071, 80), Test: (1531, 80)\n",
            "y_train: 0=1310, 1=1189\n",
            "y_val:   0=561, 1=510\n",
            "y_test:  0=803, 1=728\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled_all_features = scaler.fit_transform(X_before_selection)\n",
        "X_scaled_all_features = np.nan_to_num(X_scaled_all_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "NUM_FEATURES_TO_SELECT = 80\n",
        "print(f\"ĞÑ‚Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· {X_scaled_all_features.shape[1]} Ğ´Ğ¾ {NUM_FEATURES_TO_SELECT}...\")\n",
        "\n",
        "selector = SelectFromModel(\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    max_features=NUM_FEATURES_TO_SELECT,\n",
        "    threshold=-np.inf\n",
        ")\n",
        "\n",
        "selector.fit(X_scaled_all_features, y_labels)\n",
        "X_selected_features = selector.transform(X_scaled_all_features)\n",
        "\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X_before_selection.columns[selected_feature_indices].tolist()\n",
        "\n",
        "print(f\"ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ°: {X_selected_features.shape[1]}\")\n",
        "if 0 < X_selected_features.shape[1] <= 10: # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸Ğ¼ĞµĞ½Ğ°, ĞµÑĞ»Ğ¸ Ğ¸Ñ… Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾\n",
        "    print(f\"ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {selected_feature_names[:min(10, len(selected_feature_names))]}\")\n",
        "elif X_selected_features.shape[1] > 10:\n",
        "     print(f\"ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… 5 Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²: {selected_feature_names[:5]}\")\n",
        "\n",
        "\n",
        "if X_selected_features.shape[1] == 0:\n",
        "    raise ValueError(\"ĞÑ‚Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğµ Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ» Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ SelectFromModel.\")\n",
        "if X_selected_features.shape[1] != NUM_FEATURES_TO_SELECT:\n",
        "    print(f\"Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ {X_selected_features.shape[1]} Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ…Ğ¾Ñ‚Ñ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ğ»Ğ¾ÑÑŒ {NUM_FEATURES_TO_SELECT}. \"\n",
        "          \"Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾Ğ¹Ñ‚Ğ¸, ĞµÑĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¸Ğ»Ğ¸ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ.\")\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_selected_features, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº:\")\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"y_train: 0={np.sum(y_train==0)}, 1={np.sum(y_train==1)}\")\n",
        "print(f\"y_val:   0={np.sum(y_val==0)}, 1={np.sum(y_val==1)}\")\n",
        "print(f\"y_test:  0={np.sum(y_test==0)}, 1={np.sum(y_test==1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrNFWUgGAbi2"
      },
      "source": [
        "# AI Create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_yMBE-oA37z"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkmiaQkwA3XH",
        "outputId": "adab24f7-2951-4317-b72e-9f57314f2350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ ĞĞĞ¡ĞĞœĞ‘Ğ›Ğ¯ Ğ¡ MODEL_SEED = 5\n",
            "ğŸš€ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ (SEED=5)...\n",
            "Epoch 1/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.5049 - loss: 2.2234 - val_accuracy: 0.4883 - val_loss: 1.3656 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4997 - loss: 1.9665 - val_accuracy: 0.4809 - val_loss: 1.3951 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4825 - loss: 1.8636 - val_accuracy: 0.4837 - val_loss: 1.4103 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4956 - loss: 1.8144 - val_accuracy: 0.4865 - val_loss: 1.4097 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4948 - loss: 1.7618 - val_accuracy: 0.4809 - val_loss: 1.4100 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4816 - loss: 1.7579 - val_accuracy: 0.4790 - val_loss: 1.4112 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4717 - loss: 1.7403 - val_accuracy: 0.4725 - val_loss: 1.4013 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4711 - loss: 1.6983 - val_accuracy: 0.4771 - val_loss: 1.3911 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4820 - loss: 1.6905 - val_accuracy: 0.4809 - val_loss: 1.3799 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4724 - loss: 1.6674 - val_accuracy: 0.4809 - val_loss: 1.3662 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4859 - loss: 1.6650 - val_accuracy: 0.4818 - val_loss: 1.3581 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4949 - loss: 1.6226 - val_accuracy: 0.4846 - val_loss: 1.3416 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4864 - loss: 1.6201 - val_accuracy: 0.4911 - val_loss: 1.3283 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4967 - loss: 1.6072 - val_accuracy: 0.4986 - val_loss: 1.3220 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4878 - loss: 1.5872 - val_accuracy: 0.5023 - val_loss: 1.3167 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5000 - loss: 1.5760 - val_accuracy: 0.5135 - val_loss: 1.3051 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4973 - loss: 1.5487 - val_accuracy: 0.5210 - val_loss: 1.2895 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5025 - loss: 1.5373 - val_accuracy: 0.5266 - val_loss: 1.2796 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5144 - loss: 1.5109 - val_accuracy: 0.5210 - val_loss: 1.2689 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5321 - loss: 1.4846 - val_accuracy: 0.5257 - val_loss: 1.2578 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5426 - loss: 1.4738 - val_accuracy: 0.5294 - val_loss: 1.2575 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5643 - loss: 1.4414 - val_accuracy: 0.5406 - val_loss: 1.2292 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5758 - loss: 1.4174 - val_accuracy: 0.5481 - val_loss: 1.2265 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5774 - loss: 1.4320 - val_accuracy: 0.5453 - val_loss: 1.2138 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5870 - loss: 1.4107 - val_accuracy: 0.5518 - val_loss: 1.2154 - learning_rate: 0.0010\n",
            "Epoch 26/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5929 - loss: 1.3733 - val_accuracy: 0.5612 - val_loss: 1.2111 - learning_rate: 0.0010\n",
            "Epoch 27/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6284 - loss: 1.3337 - val_accuracy: 0.5537 - val_loss: 1.2096 - learning_rate: 0.0010\n",
            "Epoch 28/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6368 - loss: 1.3106 - val_accuracy: 0.5574 - val_loss: 1.2205 - learning_rate: 0.0010\n",
            "Epoch 29/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6591 - loss: 1.2906 - val_accuracy: 0.5630 - val_loss: 1.2077 - learning_rate: 0.0010\n",
            "Epoch 30/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6889 - loss: 1.2708 - val_accuracy: 0.5490 - val_loss: 1.2310 - learning_rate: 0.0010\n",
            "Epoch 31/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6784 - loss: 1.2398 - val_accuracy: 0.5584 - val_loss: 1.2054 - learning_rate: 0.0010\n",
            "Epoch 32/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7136 - loss: 1.2083 - val_accuracy: 0.5733 - val_loss: 1.2002 - learning_rate: 0.0010\n",
            "Epoch 33/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7027 - loss: 1.2025 - val_accuracy: 0.5602 - val_loss: 1.2146 - learning_rate: 0.0010\n",
            "Epoch 34/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7061 - loss: 1.2049 - val_accuracy: 0.5602 - val_loss: 1.2154 - learning_rate: 0.0010\n",
            "Epoch 35/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7156 - loss: 1.1573 - val_accuracy: 0.5518 - val_loss: 1.2179 - learning_rate: 0.0010\n",
            "Epoch 36/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7412 - loss: 1.1453 - val_accuracy: 0.5472 - val_loss: 1.2091 - learning_rate: 0.0010\n",
            "Epoch 37/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7554 - loss: 1.1121 - val_accuracy: 0.5705 - val_loss: 1.2216 - learning_rate: 0.0010\n",
            "Epoch 38/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7624 - loss: 1.0884 - val_accuracy: 0.5490 - val_loss: 1.2410 - learning_rate: 0.0010\n",
            "Epoch 39/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7682 - loss: 1.0708 - val_accuracy: 0.5649 - val_loss: 1.2364 - learning_rate: 0.0010\n",
            "Epoch 40/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7703 - loss: 1.0690 - val_accuracy: 0.5677 - val_loss: 1.2423 - learning_rate: 0.0010\n",
            "Epoch 41/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7648 - loss: 1.0557 - val_accuracy: 0.5630 - val_loss: 1.2558 - learning_rate: 0.0010\n",
            "Epoch 42/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7666 - loss: 1.0559 - val_accuracy: 0.5602 - val_loss: 1.2500 - learning_rate: 0.0010\n",
            "Epoch 43/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7572 - loss: 1.0476 - val_accuracy: 0.5630 - val_loss: 1.2543 - learning_rate: 0.0010\n",
            "Epoch 44/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7747 - loss: 1.0299 - val_accuracy: 0.5658 - val_loss: 1.2612 - learning_rate: 0.0010\n",
            "Epoch 45/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7926 - loss: 0.9827 - val_accuracy: 0.5528 - val_loss: 1.2896 - learning_rate: 0.0010\n",
            "Epoch 46/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7730 - loss: 1.0191 - val_accuracy: 0.5537 - val_loss: 1.2980 - learning_rate: 0.0010\n",
            "Epoch 47/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.9782 - val_accuracy: 0.5490 - val_loss: 1.2679 - learning_rate: 0.0010\n",
            "Epoch 48/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8050 - loss: 0.9589 - val_accuracy: 0.5649 - val_loss: 1.2761 - learning_rate: 0.0010\n",
            "Epoch 49/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8174 - loss: 0.9269 - val_accuracy: 0.5668 - val_loss: 1.2754 - learning_rate: 0.0010\n",
            "Epoch 50/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8114 - loss: 0.9349 - val_accuracy: 0.5826 - val_loss: 1.2479 - learning_rate: 0.0010\n",
            "Epoch 51/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8268 - loss: 0.9074 - val_accuracy: 0.5640 - val_loss: 1.2831 - learning_rate: 0.0010\n",
            "Epoch 52/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8296 - loss: 0.9122 - val_accuracy: 0.5500 - val_loss: 1.3095 - learning_rate: 0.0010\n",
            "Epoch 53/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8319 - loss: 0.8745 - val_accuracy: 0.5658 - val_loss: 1.3148 - learning_rate: 0.0010\n",
            "Epoch 54/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8408 - loss: 0.8625 - val_accuracy: 0.5602 - val_loss: 1.3597 - learning_rate: 0.0010\n",
            "Epoch 55/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8464 - loss: 0.8599 - val_accuracy: 0.5584 - val_loss: 1.3287 - learning_rate: 0.0010\n",
            "Epoch 56/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8274 - loss: 0.8471 - val_accuracy: 0.5378 - val_loss: 1.3534 - learning_rate: 0.0010\n",
            "Epoch 57/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8483 - loss: 0.8363 - val_accuracy: 0.5630 - val_loss: 1.3219 - learning_rate: 0.0010\n",
            "Epoch 58/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8549 - loss: 0.7798 - val_accuracy: 0.5658 - val_loss: 1.3611 - learning_rate: 0.0010\n",
            "Epoch 59/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8580 - loss: 0.7923 - val_accuracy: 0.5621 - val_loss: 1.3662 - learning_rate: 0.0010\n",
            "Epoch 60/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8601 - loss: 0.7877 - val_accuracy: 0.5537 - val_loss: 1.3669 - learning_rate: 0.0010\n",
            "Epoch 61/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8347 - loss: 0.8458 - val_accuracy: 0.5630 - val_loss: 1.3267 - learning_rate: 0.0010\n",
            "Epoch 62/200\n",
            "\u001b[1m69/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8374 - loss: 0.8130\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8392 - loss: 0.8139 - val_accuracy: 0.5537 - val_loss: 1.3167 - learning_rate: 0.0010\n",
            "Epoch 63/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8638 - loss: 0.7439 - val_accuracy: 0.5668 - val_loss: 1.3127 - learning_rate: 2.0000e-04\n",
            "Epoch 64/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8846 - loss: 0.6947 - val_accuracy: 0.5640 - val_loss: 1.3269 - learning_rate: 2.0000e-04\n",
            "Epoch 65/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9054 - loss: 0.6559 - val_accuracy: 0.5677 - val_loss: 1.3434 - learning_rate: 2.0000e-04\n",
            "Epoch 66/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8887 - loss: 0.7019 - val_accuracy: 0.5658 - val_loss: 1.3778 - learning_rate: 2.0000e-04\n",
            "Epoch 67/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8778 - loss: 0.6808 - val_accuracy: 0.5668 - val_loss: 1.3799 - learning_rate: 2.0000e-04\n",
            "Epoch 68/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8946 - loss: 0.6677 - val_accuracy: 0.5602 - val_loss: 1.3881 - learning_rate: 2.0000e-04\n",
            "Epoch 69/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8905 - loss: 0.6546 - val_accuracy: 0.5677 - val_loss: 1.4067 - learning_rate: 2.0000e-04\n",
            "Epoch 70/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8955 - loss: 0.6785 - val_accuracy: 0.5621 - val_loss: 1.4117 - learning_rate: 2.0000e-04\n",
            "Epoch 71/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9095 - loss: 0.6311 - val_accuracy: 0.5574 - val_loss: 1.4338 - learning_rate: 2.0000e-04\n",
            "Epoch 72/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9020 - loss: 0.6337 - val_accuracy: 0.5574 - val_loss: 1.4314 - learning_rate: 2.0000e-04\n",
            "Epoch 73/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9118 - loss: 0.6117 - val_accuracy: 0.5630 - val_loss: 1.4340 - learning_rate: 2.0000e-04\n",
            "Epoch 74/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9101 - loss: 0.6163 - val_accuracy: 0.5705 - val_loss: 1.4356 - learning_rate: 2.0000e-04\n",
            "Epoch 75/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9179 - loss: 0.5997 - val_accuracy: 0.5686 - val_loss: 1.4677 - learning_rate: 2.0000e-04\n",
            "Epoch 76/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9096 - loss: 0.6041 - val_accuracy: 0.5649 - val_loss: 1.4753 - learning_rate: 2.0000e-04\n",
            "Epoch 77/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9159 - loss: 0.6115 - val_accuracy: 0.5574 - val_loss: 1.5056 - learning_rate: 2.0000e-04\n",
            "Epoch 78/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9153 - loss: 0.5920 - val_accuracy: 0.5668 - val_loss: 1.4958 - learning_rate: 2.0000e-04\n",
            "Epoch 79/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9220 - loss: 0.5798 - val_accuracy: 0.5677 - val_loss: 1.4952 - learning_rate: 2.0000e-04\n",
            "Epoch 80/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9197 - loss: 0.5705 - val_accuracy: 0.5677 - val_loss: 1.4996 - learning_rate: 2.0000e-04\n",
            "Epoch 81/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9179 - loss: 0.5845 - val_accuracy: 0.5686 - val_loss: 1.5118 - learning_rate: 2.0000e-04\n",
            "Epoch 82/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9222 - loss: 0.5550 - val_accuracy: 0.5752 - val_loss: 1.5113 - learning_rate: 2.0000e-04\n",
            "Epoch 83/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9230 - loss: 0.5733 - val_accuracy: 0.5733 - val_loss: 1.5044 - learning_rate: 2.0000e-04\n",
            "Epoch 84/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9168 - loss: 0.5683 - val_accuracy: 0.5761 - val_loss: 1.5056 - learning_rate: 2.0000e-04\n",
            "Epoch 85/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9210 - loss: 0.5753 - val_accuracy: 0.5752 - val_loss: 1.5114 - learning_rate: 2.0000e-04\n",
            "Epoch 86/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9231 - loss: 0.5574 - val_accuracy: 0.5798 - val_loss: 1.5270 - learning_rate: 2.0000e-04\n",
            "Epoch 87/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9228 - loss: 0.5496 - val_accuracy: 0.5798 - val_loss: 1.5368 - learning_rate: 2.0000e-04\n",
            "Epoch 88/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9180 - loss: 0.5566 - val_accuracy: 0.5789 - val_loss: 1.5547 - learning_rate: 2.0000e-04\n",
            "Epoch 89/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9256 - loss: 0.5437 - val_accuracy: 0.5817 - val_loss: 1.5600 - learning_rate: 2.0000e-04\n",
            "Epoch 90/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9304 - loss: 0.5148 - val_accuracy: 0.5808 - val_loss: 1.5625 - learning_rate: 2.0000e-04\n",
            "Epoch 91/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9258 - loss: 0.5586 - val_accuracy: 0.5780 - val_loss: 1.5528 - learning_rate: 2.0000e-04\n",
            "Epoch 92/200\n",
            "\u001b[1m76/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9304 - loss: 0.5532\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9308 - loss: 0.5526 - val_accuracy: 0.5854 - val_loss: 1.5512 - learning_rate: 2.0000e-04\n",
            "Epoch 93/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9325 - loss: 0.5227 - val_accuracy: 0.5845 - val_loss: 1.5642 - learning_rate: 4.0000e-05\n",
            "Epoch 94/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9306 - loss: 0.5367 - val_accuracy: 0.5808 - val_loss: 1.5697 - learning_rate: 4.0000e-05\n",
            "Epoch 95/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9293 - loss: 0.5370 - val_accuracy: 0.5770 - val_loss: 1.5686 - learning_rate: 4.0000e-05\n",
            "Epoch 96/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9271 - loss: 0.5404 - val_accuracy: 0.5798 - val_loss: 1.5769 - learning_rate: 4.0000e-05\n",
            "Epoch 97/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9274 - loss: 0.5428 - val_accuracy: 0.5817 - val_loss: 1.5740 - learning_rate: 4.0000e-05\n",
            "Epoch 98/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9275 - loss: 0.5349 - val_accuracy: 0.5808 - val_loss: 1.5751 - learning_rate: 4.0000e-05\n",
            "Epoch 99/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9374 - loss: 0.5131 - val_accuracy: 0.5826 - val_loss: 1.5736 - learning_rate: 4.0000e-05\n",
            "Epoch 100/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9343 - loss: 0.5121 - val_accuracy: 0.5826 - val_loss: 1.5783 - learning_rate: 4.0000e-05\n",
            "Epoch 101/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9343 - loss: 0.5300 - val_accuracy: 0.5808 - val_loss: 1.5748 - learning_rate: 4.0000e-05\n",
            "Epoch 102/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9275 - loss: 0.5188 - val_accuracy: 0.5845 - val_loss: 1.5812 - learning_rate: 4.0000e-05\n",
            "Epoch 103/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9358 - loss: 0.5265 - val_accuracy: 0.5789 - val_loss: 1.5722 - learning_rate: 4.0000e-05\n",
            "Epoch 104/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9406 - loss: 0.5194 - val_accuracy: 0.5798 - val_loss: 1.5802 - learning_rate: 4.0000e-05\n",
            "Epoch 105/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9239 - loss: 0.5452 - val_accuracy: 0.5826 - val_loss: 1.5799 - learning_rate: 4.0000e-05\n",
            "Epoch 106/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9382 - loss: 0.5064 - val_accuracy: 0.5780 - val_loss: 1.5804 - learning_rate: 4.0000e-05\n",
            "Epoch 107/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9323 - loss: 0.5181 - val_accuracy: 0.5798 - val_loss: 1.5842 - learning_rate: 4.0000e-05\n",
            "Epoch 108/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9301 - loss: 0.5210 - val_accuracy: 0.5798 - val_loss: 1.5888 - learning_rate: 4.0000e-05\n",
            "Epoch 109/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9295 - loss: 0.5330 - val_accuracy: 0.5780 - val_loss: 1.5922 - learning_rate: 4.0000e-05\n",
            "Epoch 110/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9412 - loss: 0.4872 - val_accuracy: 0.5798 - val_loss: 1.5871 - learning_rate: 4.0000e-05\n",
            "Epoch 111/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9295 - loss: 0.5196 - val_accuracy: 0.5808 - val_loss: 1.5996 - learning_rate: 4.0000e-05\n",
            "Epoch 112/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9377 - loss: 0.5145 - val_accuracy: 0.5761 - val_loss: 1.5907 - learning_rate: 4.0000e-05\n",
            "Epoch 113/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9371 - loss: 0.4898 - val_accuracy: 0.5798 - val_loss: 1.5820 - learning_rate: 4.0000e-05\n",
            "Epoch 114/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9391 - loss: 0.4826 - val_accuracy: 0.5752 - val_loss: 1.5899 - learning_rate: 4.0000e-05\n",
            "Epoch 115/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9398 - loss: 0.5058 - val_accuracy: 0.5742 - val_loss: 1.5878 - learning_rate: 4.0000e-05\n",
            "Epoch 116/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9378 - loss: 0.4840 - val_accuracy: 0.5770 - val_loss: 1.5940 - learning_rate: 4.0000e-05\n",
            "Epoch 117/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9375 - loss: 0.5056 - val_accuracy: 0.5761 - val_loss: 1.6002 - learning_rate: 4.0000e-05\n",
            "Epoch 118/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9403 - loss: 0.4993 - val_accuracy: 0.5770 - val_loss: 1.6040 - learning_rate: 4.0000e-05\n",
            "Epoch 119/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9408 - loss: 0.5015 - val_accuracy: 0.5770 - val_loss: 1.6027 - learning_rate: 4.0000e-05\n",
            "Epoch 120/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9374 - loss: 0.5031 - val_accuracy: 0.5780 - val_loss: 1.6141 - learning_rate: 4.0000e-05\n",
            "Epoch 121/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9292 - loss: 0.5141 - val_accuracy: 0.5752 - val_loss: 1.6108 - learning_rate: 4.0000e-05\n",
            "Epoch 122/200\n",
            "\u001b[1m73/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9416 - loss: 0.4895\n",
            "Epoch 122: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9421 - loss: 0.4890 - val_accuracy: 0.5761 - val_loss: 1.6194 - learning_rate: 4.0000e-05\n",
            "Epoch 123/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9425 - loss: 0.4839 - val_accuracy: 0.5761 - val_loss: 1.6115 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9475 - loss: 0.4775 - val_accuracy: 0.5770 - val_loss: 1.6101 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9368 - loss: 0.5102 - val_accuracy: 0.5798 - val_loss: 1.6253 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9343 - loss: 0.5130 - val_accuracy: 0.5780 - val_loss: 1.6180 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9417 - loss: 0.4858 - val_accuracy: 0.5798 - val_loss: 1.6105 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9443 - loss: 0.4919 - val_accuracy: 0.5798 - val_loss: 1.6089 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9323 - loss: 0.5199 - val_accuracy: 0.5808 - val_loss: 1.6196 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9369 - loss: 0.4771 - val_accuracy: 0.5817 - val_loss: 1.6194 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9497 - loss: 0.4833 - val_accuracy: 0.5826 - val_loss: 1.6119 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9399 - loss: 0.4872 - val_accuracy: 0.5808 - val_loss: 1.6166 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9413 - loss: 0.4729 - val_accuracy: 0.5798 - val_loss: 1.6153 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9381 - loss: 0.4919 - val_accuracy: 0.5808 - val_loss: 1.6173 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9299 - loss: 0.5040 - val_accuracy: 0.5817 - val_loss: 1.6189 - learning_rate: 1.0000e-05\n",
            "Epoch 136/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9404 - loss: 0.4916 - val_accuracy: 0.5808 - val_loss: 1.6183 - learning_rate: 1.0000e-05\n",
            "Epoch 137/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9336 - loss: 0.4917 - val_accuracy: 0.5780 - val_loss: 1.6218 - learning_rate: 1.0000e-05\n",
            "Epoch 138/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9422 - loss: 0.4987 - val_accuracy: 0.5798 - val_loss: 1.6139 - learning_rate: 1.0000e-05\n",
            "Epoch 139/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9478 - loss: 0.4804 - val_accuracy: 0.5770 - val_loss: 1.6220 - learning_rate: 1.0000e-05\n",
            "Epoch 140/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9359 - loss: 0.5070 - val_accuracy: 0.5770 - val_loss: 1.6166 - learning_rate: 1.0000e-05\n",
            "Epoch 141/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9421 - loss: 0.4772 - val_accuracy: 0.5752 - val_loss: 1.6210 - learning_rate: 1.0000e-05\n",
            "Epoch 142/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9399 - loss: 0.4834 - val_accuracy: 0.5780 - val_loss: 1.6201 - learning_rate: 1.0000e-05\n",
            "Epoch 143/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9422 - loss: 0.4823 - val_accuracy: 0.5770 - val_loss: 1.6222 - learning_rate: 1.0000e-05\n",
            "Epoch 144/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9302 - loss: 0.4935 - val_accuracy: 0.5798 - val_loss: 1.6246 - learning_rate: 1.0000e-05\n",
            "Epoch 145/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9461 - loss: 0.4688 - val_accuracy: 0.5752 - val_loss: 1.6251 - learning_rate: 1.0000e-05\n",
            "Epoch 146/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9463 - loss: 0.4671 - val_accuracy: 0.5780 - val_loss: 1.6161 - learning_rate: 1.0000e-05\n",
            "Epoch 147/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9412 - loss: 0.4824 - val_accuracy: 0.5770 - val_loss: 1.6213 - learning_rate: 1.0000e-05\n",
            "Epoch 148/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9444 - loss: 0.4761 - val_accuracy: 0.5752 - val_loss: 1.6266 - learning_rate: 1.0000e-05\n",
            "Epoch 149/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9390 - loss: 0.4884 - val_accuracy: 0.5742 - val_loss: 1.6184 - learning_rate: 1.0000e-05\n",
            "Epoch 150/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9400 - loss: 0.4732 - val_accuracy: 0.5752 - val_loss: 1.6250 - learning_rate: 1.0000e-05\n",
            "Epoch 151/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9374 - loss: 0.5035 - val_accuracy: 0.5742 - val_loss: 1.6231 - learning_rate: 1.0000e-05\n",
            "Epoch 152/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9372 - loss: 0.4847 - val_accuracy: 0.5742 - val_loss: 1.6184 - learning_rate: 1.0000e-05\n",
            "Epoch 153/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9406 - loss: 0.4692 - val_accuracy: 0.5789 - val_loss: 1.6249 - learning_rate: 1.0000e-05\n",
            "Epoch 154/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9399 - loss: 0.4955 - val_accuracy: 0.5761 - val_loss: 1.6251 - learning_rate: 1.0000e-05\n",
            "Epoch 155/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9392 - loss: 0.4860 - val_accuracy: 0.5761 - val_loss: 1.6293 - learning_rate: 1.0000e-05\n",
            "Epoch 156/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9555 - loss: 0.4570 - val_accuracy: 0.5752 - val_loss: 1.6286 - learning_rate: 1.0000e-05\n",
            "Epoch 157/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9385 - loss: 0.5134 - val_accuracy: 0.5761 - val_loss: 1.6265 - learning_rate: 1.0000e-05\n",
            "Epoch 158/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9368 - loss: 0.4850 - val_accuracy: 0.5761 - val_loss: 1.6228 - learning_rate: 1.0000e-05\n",
            "Epoch 159/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9485 - loss: 0.4701 - val_accuracy: 0.5770 - val_loss: 1.6207 - learning_rate: 1.0000e-05\n",
            "Epoch 160/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9468 - loss: 0.4760 - val_accuracy: 0.5780 - val_loss: 1.6267 - learning_rate: 1.0000e-05\n",
            "Epoch 161/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9498 - loss: 0.4591 - val_accuracy: 0.5761 - val_loss: 1.6280 - learning_rate: 1.0000e-05\n",
            "Epoch 162/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9416 - loss: 0.4860 - val_accuracy: 0.5789 - val_loss: 1.6322 - learning_rate: 1.0000e-05\n",
            "Epoch 163/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9423 - loss: 0.4798 - val_accuracy: 0.5770 - val_loss: 1.6294 - learning_rate: 1.0000e-05\n",
            "Epoch 164/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9426 - loss: 0.4923 - val_accuracy: 0.5780 - val_loss: 1.6385 - learning_rate: 1.0000e-05\n",
            "Epoch 165/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9436 - loss: 0.4922 - val_accuracy: 0.5770 - val_loss: 1.6332 - learning_rate: 1.0000e-05\n",
            "Epoch 166/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9396 - loss: 0.4741 - val_accuracy: 0.5752 - val_loss: 1.6342 - learning_rate: 1.0000e-05\n",
            "Epoch 167/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9485 - loss: 0.4483 - val_accuracy: 0.5770 - val_loss: 1.6394 - learning_rate: 1.0000e-05\n",
            "Epoch 168/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9385 - loss: 0.4755 - val_accuracy: 0.5761 - val_loss: 1.6341 - learning_rate: 1.0000e-05\n",
            "Epoch 169/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9476 - loss: 0.4704 - val_accuracy: 0.5780 - val_loss: 1.6352 - learning_rate: 1.0000e-05\n",
            "Epoch 170/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9469 - loss: 0.4641 - val_accuracy: 0.5770 - val_loss: 1.6292 - learning_rate: 1.0000e-05\n",
            "Epoch 171/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9471 - loss: 0.4725 - val_accuracy: 0.5770 - val_loss: 1.6377 - learning_rate: 1.0000e-05\n",
            "Epoch 172/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9422 - loss: 0.4618 - val_accuracy: 0.5789 - val_loss: 1.6342 - learning_rate: 1.0000e-05\n",
            "Epoch 173/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9407 - loss: 0.4726 - val_accuracy: 0.5798 - val_loss: 1.6410 - learning_rate: 1.0000e-05\n",
            "Epoch 174/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9471 - loss: 0.4564 - val_accuracy: 0.5798 - val_loss: 1.6378 - learning_rate: 1.0000e-05\n",
            "Epoch 175/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9381 - loss: 0.4970 - val_accuracy: 0.5798 - val_loss: 1.6325 - learning_rate: 1.0000e-05\n",
            "Epoch 176/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9421 - loss: 0.4808 - val_accuracy: 0.5761 - val_loss: 1.6265 - learning_rate: 1.0000e-05\n",
            "Epoch 177/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9455 - loss: 0.4757 - val_accuracy: 0.5761 - val_loss: 1.6273 - learning_rate: 1.0000e-05\n",
            "Epoch 178/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9488 - loss: 0.4623 - val_accuracy: 0.5789 - val_loss: 1.6353 - learning_rate: 1.0000e-05\n",
            "Epoch 179/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9478 - loss: 0.4663 - val_accuracy: 0.5789 - val_loss: 1.6406 - learning_rate: 1.0000e-05\n",
            "Epoch 180/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9513 - loss: 0.4638 - val_accuracy: 0.5798 - val_loss: 1.6432 - learning_rate: 1.0000e-05\n",
            "Epoch 181/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9431 - loss: 0.4671 - val_accuracy: 0.5798 - val_loss: 1.6499 - learning_rate: 1.0000e-05\n",
            "Epoch 182/200\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9512 - loss: 0.4417 - val_accuracy: 0.5798 - val_loss: 1.6464 - learning_rate: 1.0000e-05\n",
            "Epoch 182: early stopping\n",
            "Restoring model weights from the end of the best epoch: 92.\n",
            "Max val_accuracy Ğ´Ğ»Ñ SEED=5: 0.5854\n",
            "Test accuracy Ğ´Ğ»Ñ SEED=5: 0.6003\n",
            "Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ: 1388/1531 (90.7%)\n",
            "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² saved_ensemble_models/ensemble_model_S5_test_acc_0.6003_val_acc_0.5854.keras ...\n",
            "ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "MODEL_SEED = 5\n",
        "np.random.seed(MODEL_SEED)\n",
        "tf.random.set_seed(MODEL_SEED)\n",
        "print(f\"ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ ĞĞĞ¡ĞĞœĞ‘Ğ›Ğ¯ Ğ¡ MODEL_SEED = {MODEL_SEED}\")\n",
        "# -------------------------------------------------------\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.35),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.35),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(32, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(16, activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0015)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "class_weight = {0: 1.0, 1: 2.5}\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_save_dir = \"saved_ensemble_models\"\n",
        "if not os.path.exists(model_save_dir):\n",
        "    os.makedirs(model_save_dir)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=90,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=30,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"ğŸš€ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ (SEED={MODEL_SEED})...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "max_val_acc_this_run = 0\n",
        "if hasattr(history, 'history') and history.history and 'val_accuracy' in history.history:\n",
        "    max_val_acc_this_run = max(history.history['val_accuracy'])\n",
        "print(f\"Max val_accuracy Ğ´Ğ»Ñ SEED={MODEL_SEED}: {max_val_acc_this_run:.4f}\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy Ğ´Ğ»Ñ SEED={MODEL_SEED}: {test_acc:.4f}\")\n",
        "\n",
        "test_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
        "confident_predictions = np.sum((test_pred_proba > 0.7) | (test_pred_proba < 0.3))\n",
        "print(f\"Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ: {confident_predictions}/{len(test_pred_proba)} ({confident_predictions/len(test_pred_proba)*100:.1f}%)\")\n",
        "\n",
        "model_filename = f\"ensemble_model_S{MODEL_SEED}_test_acc_{test_acc:.4f}_val_acc_{max_val_acc_this_run:.4f}.keras\"\n",
        "filepath_to_save = os.path.join(model_save_dir, model_filename)\n",
        "print(f\"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² {filepath_to_save} ...\")\n",
        "model.save(filepath_to_save)\n",
        "print(\"ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°.\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = [\n",
        "    \"saved_ensemble_models/ensemble_model_S10_test_acc_0.5996_val_acc_0.5752.keras\",\n",
        "    \"saved_ensemble_models/ensemble_model_S3_test_acc_0.6042_val_acc_0.5798.keras\",\n",
        "]\n",
        "\n",
        "all_probas = []\n",
        "\n",
        "for model_path in model_paths:\n",
        "    print(f\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ: {model_path}\")\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    probas = model.predict(X_test, verbose=0).flatten()\n",
        "    all_probas.append(probas)\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "stacked_probas = np.stack(all_probas, axis=0)\n",
        "mean_probas = np.mean(stacked_probas, axis=0)\n",
        "\n",
        "ensemble_preds = (mean_probas > 0.5).astype(int)\n",
        "\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f\"\\nEnsemble Test Accuracy (average probability, {len(model_paths)} models): {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
        "\n",
        "confident_ensemble_predictions = np.sum((mean_probas > 0.7) | (mean_probas < 0.3))\n",
        "print(f\"Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ: {confident_ensemble_predictions}/{len(mean_probas)} ({confident_ensemble_predictions/len(mean_probas)*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3NJgSACXgzm",
        "outputId": "48fd413b-c99a-445b-d03c-429b99030d23"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ: saved_ensemble_models/ensemble_model_S10_test_acc_0.5996_val_acc_0.5752.keras\n",
            "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ: saved_ensemble_models/ensemble_model_S3_test_acc_0.6042_val_acc_0.5798.keras\n",
            "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ: saved_ensemble_models/ensemble_model_S5_test_acc_0.5800_val_acc_0.5882.keras\n",
            "\n",
            "Ensemble Test Accuracy (average probability, 3 models): 0.5872 (58.72%)\n",
            "Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ: 1010/1531 (66.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5s6HmNlOQNU"
      },
      "source": [
        "# Upload real data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAGwL4G9n3S"
      },
      "source": [
        "## Extracting Features from JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "aFBedrTy9qRD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "ac1f6947-5bcf-48fc-fa77-3d1ae78db003"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fot_test_500.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-5b39492032db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_real_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fot_test_500.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_real_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_real_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'Unnamed:'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprice_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'outcome_price'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'initial_entry_price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_potential_feature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_real_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feat_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'strat_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'composite_score_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fot_test_500.csv'"
          ]
        }
      ],
      "source": [
        "df_real_test = pd.read_csv(\"fot_test_500.csv\")\n",
        "df_real_test.drop(columns=[col for col in df_real_test.columns if 'Unnamed:' in col], inplace=True, errors='ignore')\n",
        "\n",
        "price_cols = ['outcome_price', 'initial_entry_price']\n",
        "all_potential_feature_cols = [col for col in df_real_test.columns if col.startswith('feat_') or col.startswith('strat_') or col == 'composite_score_value']\n",
        "\n",
        "if 'feat_market_state' in df_real_test.columns and df_real_test['feat_market_state'].dtype == 'object':\n",
        "    df_real_test['feat_market_state'] = df_real_test['feat_market_state'].map({'TREND': 1, 'TRANSITION': 0.5, 'FLAT': 0}).fillna(0)\n",
        "\n",
        "cols_to_convert_numeric = price_cols + all_potential_feature_cols\n",
        "for col_name in list(set(cols_to_convert_numeric)):\n",
        "    if col_name in df_real_test.columns:\n",
        "        df_real_test[col_name] = pd.to_numeric(df_real_test[col_name].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "\n",
        "cols_for_dropna_real = price_cols[:]\n",
        "if 'outcome_status' in df_real_test.columns: cols_for_dropna_real.append('outcome_status')\n",
        "df_real_test.dropna(subset=cols_for_dropna_real, inplace=True)\n",
        "if df_real_test.empty: raise ValueError(\"Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ DataFrame Ğ¿ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸.\")\n",
        "\n",
        "base_feature_columns_real = [col for col in all_potential_feature_cols if col in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test[col])]\n",
        "if 'feat_market_state' in df_real_test.columns and pd.api.types.is_numeric_dtype(df_real_test['feat_market_state']) and 'feat_market_state' not in base_feature_columns_real:\n",
        "    base_feature_columns_real.append('feat_market_state')\n",
        "base_feature_columns_real = list(set(base_feature_columns_real))\n",
        "df_real_test[base_feature_columns_real] = df_real_test[base_feature_columns_real].fillna(0)\n",
        "\n",
        "new_cols_data_real = {}\n",
        "lags = [1, 2, 4, 8]; roll_windows = [4, 8]\n",
        "for col in base_feature_columns_real:\n",
        "    for lag in lags: new_cols_data_real[f'{col}_lag{lag}'] = df_real_test[col].shift(lag)\n",
        "    for window in roll_windows: new_cols_data_real[f'{col}_roll_mean{window}'] = df_real_test[col].rolling(window=window, min_periods=1).mean()\n",
        "df_real_test = pd.concat([df_real_test, pd.DataFrame(new_cols_data_real, index=df_real_test.index)], axis=1)\n",
        "df_real_test.dropna(inplace=True)\n",
        "if df_real_test.empty: raise ValueError(\"Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ DataFrame Ğ¿ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Feature Engineering.\")\n",
        "\n",
        "for col_to_check in X_before_selection_columns: # X_before_selection_columns Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ñ€Ğ°Ğ½ĞµĞµ!\n",
        "    if col_to_check not in df_real_test.columns: df_real_test[col_to_check] = 0\n",
        "X_real_test_prepared = df_real_test[X_before_selection_columns].fillna(0) # fillna(0) Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº\n",
        "\n",
        "# --- Ğ¨Ğ°Ğ³ 2: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ---\n",
        "# scaler Ğ¸ selector Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸!\n",
        "X_real_test_scaled_all = scaler.transform(X_real_test_prepared)\n",
        "X_real_test_scaled_all = np.nan_to_num(X_real_test_scaled_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_real_test_selected = selector.transform(X_real_test_scaled_all)\n",
        "\n",
        "y_real_test = (df_real_test.loc[X_real_test_prepared.index, 'outcome_status'] == 'LONG_MET').astype(int) if 'outcome_status' in df_real_test.columns and not df_real_test.empty else None\n",
        "\n",
        "# --- Ğ¨Ğ°Ğ³ 3: ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ¼ ---\n",
        "model_ensemble_paths = [ # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ ÑÑ‚Ğ¾Ñ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸ Ğº Ğ²Ğ°ÑˆĞ¸Ğ¼ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼\n",
        "    \"saved_ensemble_models/ensemble_model_S10_test_acc_0.5996_val_acc_0.5752.keras\",\n",
        "    \"saved_ensemble_models/ensemble_model_S7_test_acc_0.5918_val_acc_0.5798.keras\",\n",
        "]\n",
        "all_real_probas = []\n",
        "for model_path in model_ensemble_paths:\n",
        "    if not os.path.exists(model_path): print(f\"ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº: {model_path}\"); continue\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    all_real_probas.append(model.predict(X_real_test_selected, verbose=0).flatten())\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "if not all_real_probas: raise ValueError(\"ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹.\")\n",
        "\n",
        "mean_real_probas = np.mean(np.stack(all_real_probas, axis=0), axis=0)\n",
        "ensemble_real_preds = (mean_real_probas > 0.5).astype(int)\n",
        "\n",
        "print(f\"\\n--- Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ({len(X_real_test_selected)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹) ---\")\n",
        "for i in range(min(5, len(X_real_test_selected))):\n",
        "    pred_label = \"LONG\" if ensemble_real_preds[i] == 1 else \"SHORT\"\n",
        "    actual_val = y_real_test.iloc[i] if y_real_test is not None and i < len(y_real_test) else \"N/A\"\n",
        "    actual_label_info = f\"(Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹: {actual_val})\" if y_real_test is not None else \"\"\n",
        "    print(f\"ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ {i+1}: Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ LONG={mean_real_probas[i]:.4f}, ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ={pred_label} {actual_label_info}\")\n",
        "\n",
        "if y_real_test is not None and len(y_real_test) == len(ensemble_real_preds):\n",
        "    ensemble_real_accuracy = accuracy_score(y_real_test, ensemble_real_preds)\n",
        "    print(f\"\\nEnsemble Accuracy Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {ensemble_real_accuracy:.4f} ({ensemble_real_accuracy*100:.2f}%)\")\n",
        "    print(\"\\nClassification Report Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:\")\n",
        "    print(classification_report(y_real_test, ensemble_real_preds, target_names=['SHORT', 'LONG'], zero_division=0))\n",
        "\n",
        "    confident_ensemble_predictions = np.sum((mean_real_probas > 0.7) | (mean_real_probas < 0.3))\n",
        "    print(f\"Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ: {confident_ensemble_predictions}/{len(mean_real_probas)} ({confident_ensemble_predictions/len(mean_real_probas)*100:.1f}%)\")\n",
        "else:\n",
        "    print(\"\\nĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: y_real_test Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD8-Hrn798Bb"
      },
      "source": [
        "## Predict Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHEBi2iY9-zM",
        "outputId": "eb501211-3b02-48b7-9bf2-c20dab5ab1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X is not a NumPy array, attempting to use .columns\n",
            "ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ² X: ['strat_pattern_strength', 'strat_indicator_consensus', 'strat_indicator_strength', 'feat_vol_z_1m', 'feat_vol_z_5m', 'feat_vol_z_combined', 'feat_price_pct_change', 'feat_oi_z', 'feat_long_liq_usd', 'feat_short_liq_usd', 'feat_total_liq_usd', 'feat_cvd', 'feat_spread_pct', 'feat_trade_rate_per_min', 'feat_large_trade_ratio', 'feat_ob_imbalance', 'feat_rsi', 'feat_macd_line', 'feat_macd_signal', 'feat_macd_hist', 'feat_ema_short', 'feat_ema_long', 'feat_bb_upper', 'feat_bb_middle', 'feat_bb_lower', 'feat_bb_width_norm', 'feat_obv', 'feat_adx', 'feat_atr_norm', 'feat_atr_sma_norm', 'feat_price_slope_norm', 'feat_vwap', 'feat_funding_rate', 'feat_mark_price', 'feat_index_price', 'feat_btc_correlation', 'rsi_price_interaction', 'volume_price_ratio', 'macd_signal_strength', 'liq_imbalance']\n",
            "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ² X: 40\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 0: 0\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 1: 1\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 2: 2\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 3: 3\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 4: 4\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 5: 5\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 6: 6\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 7: 7\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 8: 8\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 9: 9\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 10: 10\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 11: 11\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 12: 12\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 13: 13\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 14: 14\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 15: 15\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 16: 16\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 17: 17\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 18: 18\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 19: 19\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 20: 20\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 21: 21\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 22: 22\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 23: 23\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 24: 24\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 25: 25\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 26: 26\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 27: 27\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 28: 28\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 29: 29\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 30: 30\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 31: 31\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 32: 32\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 33: 33\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 34: 34\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 35: 35\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 36: 36\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 37: 37\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 38: 38\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 39: 39\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 40: 40\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 41: 41\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 42: 42\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 43: 43\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 44: 44\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 45: 45\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 46: 46\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 47: 47\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 48: 48\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 49: 49\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 50: 50\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 51: 51\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 52: 52\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 53: 53\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 54: 54\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 55: 55\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 56: 56\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 57: 57\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 58: 58\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 59: 59\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 60: 60\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 61: 61\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 62: 62\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 63: 63\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 64: 64\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 65: 65\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 66: 66\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 67: 67\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 68: 68\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 69: 69\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 70: 70\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 71: 71\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 72: 72\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 73: 73\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 74: 74\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 75: 75\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 76: 76\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 77: 77\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 78: 78\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 79: 79\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 80: 80\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 81: 81\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 82: 82\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 83: 83\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 84: 84\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 85: 85\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 86: 86\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 87: 87\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 88: 88\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 89: 89\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 90: 90\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 91: 91\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 92: 92\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 93: 93\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 94: 94\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 95: 95\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 96: 96\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 97: 97\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 98: 98\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 99: 99\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 100: 100\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 101: 101\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 102: 102\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 103: 103\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 104: 104\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 105: 105\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 106: 106\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 107: 107\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 108: 108\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 109: 109\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 110: 110\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 111: 111\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 112: 112\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 113: 113\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 114: 114\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 115: 115\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 116: 116\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 117: 117\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 118: 118\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 119: 119\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 120: 120\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 121: 121\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 122: 122\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 123: 123\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 124: 124\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 125: 125\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 126: 126\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 127: 127\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 128: 128\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 129: 129\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 130: 130\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 131: 131\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 132: 132\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 133: 133\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 134: 134\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 135: 135\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 136: 136\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 137: 137\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 138: 138\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 139: 139\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 140: 140\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 141: 141\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 142: 142\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 143: 143\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 144: 144\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 145: 145\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 146: 146\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 147: 147\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 148: 148\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 149: 149\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 150: 150\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 151: 151\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 152: 152\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 153: 153\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 154: 154\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 155: 155\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 156: 156\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 157: 157\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 158: 158\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 159: 159\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 160: 160\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 161: 161\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 162: 162\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 163: 163\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 164: 164\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 165: 165\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 166: 166\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 167: 167\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 168: 168\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 169: 169\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 170: 170\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 171: 171\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 172: 172\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 173: 173\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 174: 174\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 175: 175\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 176: 176\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 177: 177\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 178: 178\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 179: 179\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 180: 180\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 181: 181\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 182: 182\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 183: 183\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 184: 184\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 185: 185\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 186: 186\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 187: 187\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 188: 188\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 189: 189\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 190: 190\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 191: 191\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 192: 192\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 193: 193\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 194: 194\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 195: 195\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 196: 196\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 197: 197\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 198: 198\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 199: 199\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 200: 200\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 201: 201\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 202: 202\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 203: 203\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 204: 204\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 205: 205\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 206: 206\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 207: 207\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 208: 208\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 209: 209\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 210: 210\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 211: 211\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 212: 212\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 213: 213\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 214: 214\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 215: 215\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 216: 216\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 217: 217\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 218: 218\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 219: 219\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 220: 220\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 221: 221\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 222: 222\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 223: 223\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 224: 224\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 225: 225\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 226: 226\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 227: 227\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 228: 228\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 229: 229\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 230: 230\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 231: 231\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 232: 232\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 233: 233\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 234: 234\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 235: 235\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 236: 236\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 237: 237\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 238: 238\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 239: 239\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 240: 240\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 241: 241\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 242: 242\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 243: 243\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 244: 244\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 245: 245\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 246: 246\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 247: 247\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 248: 248\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 249: 249\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 250: 250\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 251: 251\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 252: 252\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 253: 253\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 254: 254\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 255: 255\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 256: 256\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 257: 257\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 258: 258\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 259: 259\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 260: 260\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 261: 261\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 262: 262\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 263: 263\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 264: 264\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 265: 265\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 266: 266\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 267: 267\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 268: 268\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 269: 269\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 270: 270\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 271: 271\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 272: 272\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 273: 273\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 274: 274\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 275: 275\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 276: 276\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 277: 277\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 278: 278\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 279: 279\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 280: 280\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 281: 281\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 282: 282\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 283: 283\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 284: 284\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 285: 285\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 286: 286\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 287: 287\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 288: 288\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 289: 289\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 290: 290\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 291: 291\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 292: 292\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 293: 293\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 294: 294\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 295: 295\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 296: 296\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 297: 297\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 298: 298\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 299: 299\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 300: 300\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 301: 301\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 302: 302\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 303: 303\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 304: 304\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 305: 305\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 306: 306\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 307: 307\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 308: 308\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 309: 309\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 310: 310\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 311: 311\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 312: 312\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 313: 313\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 314: 314\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 315: 315\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 316: 316\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 317: 317\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 318: 318\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 319: 319\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 320: 320\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 321: 321\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 322: 322\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 323: 323\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 324: 324\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 325: 325\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 326: 326\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 327: 327\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 328: 328\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 329: 329\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 330: 330\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 331: 331\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 332: 332\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 333: 333\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 334: 334\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 335: 335\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 336: 336\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 337: 337\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 338: 338\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 339: 339\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 340: 340\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 341: 341\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 342: 342\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 343: 343\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 344: 344\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 345: 345\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 346: 346\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 347: 347\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 348: 348\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 349: 349\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 350: 350\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 351: 351\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 352: 352\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 353: 353\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 354: 354\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 355: 355\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 356: 356\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 357: 357\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 358: 358\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 359: 359\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 360: 360\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 361: 361\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 362: 362\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 363: 363\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 364: 364\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 365: 365\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 366: 366\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 367: 367\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 368: 368\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 369: 369\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 370: 370\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 371: 371\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 372: 372\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 373: 373\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 374: 374\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 375: 375\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 376: 376\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 377: 377\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 378: 378\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 379: 379\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 380: 380\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 381: 381\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 382: 382\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 383: 383\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 384: 384\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 385: 385\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 386: 386\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 387: 387\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 388: 388\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 389: 389\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 390: 390\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 391: 391\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 392: 392\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 393: 393\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 394: 394\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 395: 395\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 396: 396\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 397: 397\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 398: 398\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 399: 399\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 400: 400\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 401: 401\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 402: 402\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 403: 403\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 404: 404\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 405: 405\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 406: 406\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 407: 407\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 408: 408\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 409: 409\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 410: 410\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 411: 411\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 412: 412\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 413: 413\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 414: 414\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 415: 415\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 416: 416\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 417: 417\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 418: 418\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 419: 419\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 420: 420\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 421: 421\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 422: 422\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 423: 423\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 424: 424\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 425: 425\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 426: 426\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 427: 427\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 428: 428\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 429: 429\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 430: 430\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 431: 431\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 432: 432\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 433: 433\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 434: 434\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 435: 435\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 436: 436\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 437: 437\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 438: 438\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 439: 439\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 440: 440\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 441: 441\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 442: 442\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 443: 443\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 444: 444\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 445: 445\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 446: 446\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 447: 447\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 448: 448\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 449: 449\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 450: 450\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 451: 451\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 452: 452\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 453: 453\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 454: 454\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 455: 455\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 456: 456\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 457: 457\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 458: 458\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 459: 459\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 460: 460\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 461: 461\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 462: 462\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 463: 463\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 464: 464\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 465: 465\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 466: 466\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 467: 467\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 468: 468\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 469: 469\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 470: 470\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 471: 471\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 472: 472\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 473: 473\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 474: 474\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 475: 475\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 476: 476\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 477: 477\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 478: 478\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 479: 479\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 480: 480\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 481: 481\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 482: 482\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 483: 483\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 484: 484\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 485: 485\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 486: 486\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 487: 487\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 488: 488\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 489: 489\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 490: 490\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 491: 491\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 492: 492\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 493: 493\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 494: 494\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 495: 495\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 496: 496\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 497: 497\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 498: 498\n",
            "ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ 499: 499\n",
            "Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹: 0 Ğ¸Ğ· 0\n",
            "âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ 0 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² final_predictions.json\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, (np.generic,)):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, np.ndarray): # Handle ndarray conversion\n",
        "        return obj.tolist()\n",
        "    if obj == np.inf or obj == -np.inf:\n",
        "        return \"Infinity\" # Or None, or a large number string\n",
        "    if np.isnan(obj):\n",
        "        return None # Or \"NaN\"\n",
        "    return obj\n",
        "\n",
        "def format_symbol(symbol):\n",
        "    if isinstance(symbol, str):\n",
        "        return symbol.replace(\"/USDT\", \"\").upper()\n",
        "    return symbol\n",
        "\n",
        "# Corrected print statements for X\n",
        "if isinstance(X, np.ndarray):\n",
        "    print(f\"Ğ¤Ğ¾Ñ€Ğ¼Ğ° X (NumPy array): {X.shape}\")\n",
        "    if X.ndim > 1:\n",
        "        print(f\"ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº (Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²) Ğ² X: {X.shape[1]}\")\n",
        "    elif X.ndim == 1 and len(df) == X.shape[0]: # If X is 1D but per row for df (unlikely for features)\n",
        "        print(f\"X is 1D, length: {X.shape[0]}\")\n",
        "    elif X.ndim == 1: # If X is a single feature vector not matching df rows\n",
        "         print(f\"X is a single 1D feature vector, length: {X.shape[0]}\")\n",
        "\n",
        "else: # Should not happen based on the error, but good for robustness\n",
        "    print(\"X is not a NumPy array, attempting to use .columns\")\n",
        "    print(\"ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ² X:\", list(X.columns))\n",
        "    print(\"ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ² X:\", len(X.columns))\n",
        "\n",
        "\n",
        "# Assuming X and df have the same number of rows\n",
        "# and X contains the features for each row in df.\n",
        "if len(df) != X.shape[0] and isinstance(X, np.ndarray):\n",
        "    print(f\"Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Ğ² df ({len(df)}) Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑÑ‚Ñ€Ğ¾Ğº Ğ² X ({X.shape[0]})!\")\n",
        "    # Decide how to proceed or stop if this is an issue\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· X Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸\n",
        "        # X[i] will give the i-th row if X is a 2D NumPy array\n",
        "        current_row_features = X[i].reshape(1, -1)\n",
        "\n",
        "        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ\n",
        "        # It's assumed 'scaler_X' is fitted on data with the same structure as rows of X\n",
        "        model_input_scaled = scaler_X.transform(current_row_features)\n",
        "        pred_percent_change = model.predict(model_input_scaled, verbose=0).flatten()[0]\n",
        "\n",
        "        # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ\n",
        "        if pred_percent_change < -50:\n",
        "            pred_percent_change = -10.0\n",
        "        elif pred_percent_change > 50:\n",
        "            pred_percent_change = 10.0\n",
        "\n",
        "        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ\n",
        "        initial_price = float(row.get('initial_entry_price', 0))\n",
        "        actual_price = float(row.get('outcome_price', 0))\n",
        "        actual_percent_change = ((actual_price - initial_price) / initial_price * 100) if initial_price != 0 else 0\n",
        "\n",
        "\n",
        "        result = {\n",
        "            \"symbol\": format_symbol(row.get(\"symbol\", \"UNKNOWN\")),\n",
        "            \"predicted_change_percent\": float(pred_percent_change),\n",
        "            \"actual_change_percent\": float(actual_percent_change),\n",
        "            \"actual_price\": float(actual_price),\n",
        "            \"initial_price\": float(initial_price),\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {i + 1}/{len(df)}\")\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° IndexError Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ {i} (Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, X ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ, Ñ‡ĞµĞ¼ df): {e}\")\n",
        "        print(f\"Ğ”Ğ»Ğ¸Ğ½Ğ° df: {len(df)}, Ğ¤Ğ¾Ñ€Ğ¼Ğ° X: {X.shape if isinstance(X, np.ndarray) else 'ĞĞµ NumPy Ğ¼Ğ°ÑÑĞ¸Ğ²'}\")\n",
        "        continue # Or break, depending on how critical this is\n",
        "    except Exception as e:\n",
        "        print(f\"ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑÑ‚Ñ€Ğ¾ĞºĞµ {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ\n",
        "valid_results = []\n",
        "for result in results:\n",
        "    pred = result['predicted_change_percent']\n",
        "    if pred is not None and not np.isnan(pred) and not np.isinf(pred):\n",
        "        valid_results.append(result)\n",
        "\n",
        "print(f\"Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹: {len(valid_results)} Ğ¸Ğ· {len(results)}\")\n",
        "\n",
        "if valid_results:\n",
        "    predicted_changes = [r['predicted_change_percent'] for r in valid_results]\n",
        "    actual_changes = [r['actual_change_percent'] for r in valid_results]\n",
        "\n",
        "    # Filter out potential non-numeric or problematic actual_price values before finding min/max\n",
        "    valid_actual_prices = [r['actual_price'] for r in valid_results if isinstance(r['actual_price'], (int, float)) and r['actual_price'] > 0]\n",
        "\n",
        "    print(f\"Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹: {min(predicted_changes):.2f}% - {max(predicted_changes):.2f}%\")\n",
        "    print(f\"Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ: {np.mean(predicted_changes):.2f}%\")\n",
        "    if actual_changes: # Ensure actual_changes is not empty\n",
        "        print(f\"Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹: {min(actual_changes):.2f}% - {max(actual_changes):.2f}%\")\n",
        "    else:\n",
        "        print(\"ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹.\")\n",
        "\n",
        "    if valid_actual_prices:\n",
        "        print(f\"Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ½: {min(valid_actual_prices):.4f} - {max(valid_actual_prices):.4f}\")\n",
        "    else:\n",
        "        print(\"ĞĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°.\")\n",
        "\n",
        "\n",
        "# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ\n",
        "output_data = {\n",
        "    \"total_predictions\": len(valid_results),\n",
        "    \"predictions\": valid_results\n",
        "}\n",
        "\n",
        "with open(\"final_predictions.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False, default=convert_numpy_types)\n",
        "\n",
        "print(f\"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {len(valid_results)} Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² final_predictions.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WOw8yMd1VlnD",
        "NvUGC8QQV6bV",
        "fhYaZ-ENV_c5",
        "3abSxRqvWEIB",
        "MrNFWUgGAbi2",
        "L5s6HmNlOQNU",
        "RyAGwL4G9n3S"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}